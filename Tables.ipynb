{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables for FairCal based on Salvador et al., 2022"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables Reproduced\n",
    "\n",
    "1. Global Accuracy Measures\n",
    "2. Fairness Calibration\n",
    "3. Predictive Equality ('fpr at fpr')\n",
    "4. Equal Opportunity ('fnr at fnr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Manager to load pickle files and provide data based on specific table of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager:\n",
    "    '''\n",
    "    This class stores the information about the data corresponding to a specific dataset, feature, approach, and objective combination, among other factors.\n",
    "    Input: \n",
    "        dataset (str), e.g. 'rfw'\n",
    "        feature (str), e.g. 'facenet'\n",
    "        approach (str), e.g. 'faircal'\n",
    "        objective (str), e.g. 'accuracy'\n",
    "        ...\n",
    "        \n",
    "    Methods\n",
    "        load_pickle: loads a pickle file into a results dictionary\n",
    "        provide_data: returns a dataframe based on pre-specified objective and other self attributes\n",
    "        get_sensitive_attributes_subgroups: returns a list of sensitive attributes and a dictionary with keys = sensitive attributes and values = subgroups imposed by that attribute\n",
    "\n",
    "    '''\n",
    "    def __init__(self, dataset, feature, approach, objective, calibration_method='beta', n_clusters=100, measure='ece', at_error='1e-2', subgroup='African', fpr_def=[1e-3, 1e-2]):\n",
    "        self.dataset = dataset\n",
    "        self.feature = feature\n",
    "        self.approach = approach\n",
    "        self.objective = objective\n",
    "        self.calibration_method = calibration_method\n",
    "        self.n_clusters = n_clusters\n",
    "        self.measure = measure\n",
    "        self.at_error = at_error\n",
    "        self.subgroup = subgroup\n",
    "        self.fpr_def = fpr_def\n",
    "        self.errors = 'fpr at fpr' if objective == 'predictive_equality' else 'fnr at fnr'\n",
    "        self.nbins = 25 if dataset == 'bfw' else 10\n",
    "        self.att = 'att' if dataset == 'bfw' else 'ethnicity'\n",
    "        self.key = 'calibration' if approach in ['faircal', 'baseline', 'faircal-gmm', 'oracle'] else 'pre_calibration'\n",
    "        self.sensitive_attributes, self.subgroups = self.get_sensitive_attributes_subgroups(self.dataset)\n",
    "        self.load_pickle()        \n",
    "\n",
    "    def load_pickle(self):\n",
    "        \"\"\" Load files \"\"\"\n",
    "        filename = f'./experiments/{self.dataset}/{self.feature}/{self.approach}/{self.calibration_method}/nbins_{self.nbins}'\n",
    "        if self.approach in ['faircal', 'faircal-gmm'] :\n",
    "            filename += f'_nclusters_{self.n_clusters}'\n",
    "        if self.approach == 'fsn':\n",
    "            filename += f'_nclusters_{self.n_clusters}_fpr_1e-03'\n",
    "        self.results = np.load(f'{filename}.npy', allow_pickle=True).item()\n",
    "\n",
    "    def provide_data(self, objective):\n",
    "        \"\"\" Collect appropriate data based on objective\"\"\"\n",
    "        data = pd.DataFrame()\n",
    "        data['folds'] = ['fold1', 'fold2', 'fold3', 'fold4', 'fold5']\n",
    "        data = data.set_index('folds')\n",
    "        \n",
    "        if self.objective == 'accuracy':\n",
    "            data['AUROC'] = np.nan\n",
    "            data['TPR @ 0.1% FPR'] = np.nan\n",
    "            data['TPR @ 1% FPR'] = np.nan\n",
    "            for fold in range(1,6):\n",
    "                fpr = self.results['fold'+str(fold)]['fpr'][self.att]['Global'][self.key]\n",
    "                tpr = self.results['fold'+str(fold)]['tpr'][self.att]['Global'][self.key]\n",
    "                data.loc[f'fold{str(fold)}', 'AUROC'] = sklearn.metrics.auc(fpr,tpr)\n",
    "                inter = np.interp(self.fpr_def, fpr, tpr)\n",
    "                data.iloc[fold-1, 1:] = inter\n",
    "        \n",
    "        elif self.objective == 'fairness_calibration':\n",
    "            for fold in range(1, 6):\n",
    "                for sensitive_attribute in self.sensitive_attributes:\n",
    "                    for subgroup in self.subgroups[sensitive_attribute]:\n",
    "                        data.loc[f'fold{str(fold)}', f'{subgroup}'] = self.results[f'fold{str(fold)}'][self.measure][sensitive_attribute][subgroup]\n",
    "      \n",
    "        elif self.objective == 'predictive_equality' or objective == 'equal_opportunity':\n",
    "            data[self.at_error] = np.nan\n",
    "            \n",
    "            for fold in range(1,6):\n",
    "                if self.subgroup == 'Global':\n",
    "                    data.iloc[fold-1,:] = self.at_error\n",
    "                else:\n",
    "                    fpr_global = self.results['fold'+str(fold)]['fpr'][self.att]['Global'][self.key]\n",
    "                    tpr_global = self.results['fold'+str(fold)]['tpr'][self.att]['Global'][self.key]\n",
    "                    thr_global = np.fmin(self.results['fold'+str(fold)]['thresholds'][self.att]['Global'][self.key], 1)\n",
    "\n",
    "                    fpr = self.results['fold'+str(fold)]['fpr'][self.att][self.subgroup][self.key]\n",
    "                    tpr = self.results['fold'+str(fold)]['tpr'][self.att][self.subgroup][self.key]\n",
    "                    thr = np.fmin(self.results['fold'+str(fold)]['thresholds'][self.att][self.subgroup][self.key], 1)\n",
    "\n",
    "                    if self.errors == 'fpr at fpr':\n",
    "                        thr_at_error = np.interp(self.at_error,fpr_global,thr_global)\n",
    "                        data.iloc[fold-1,:] = np.interp(thr_at_error,thr[::-1],fpr[::-1])\n",
    "                    elif self.errors == 'fnr at fnr':\n",
    "                        thr_at_error = np.interp(1-np.array(self.at_error),tpr_global,thr_global)\n",
    "                        data.iloc[fold-1,:] = 1-np.interp(thr_at_error,thr[::-1],tpr[::-1])  \n",
    "        else:\n",
    "            print('Please specify a valid objective.')\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sensitive_attributes_subgroups(dataset):\n",
    "        '''\n",
    "        A helper function to get sensitive attributes per subgroup, depending on the dataset.\n",
    "        \n",
    "        '''\n",
    "        if dataset == 'rfw':\n",
    "            sensitive_attributes = ['ethnicity']\n",
    "            subgroups = {'ethnicity':['African', 'Asian', 'Caucasian', 'Indian']}\n",
    "        else:\n",
    "            sensitive_attributes = ['e', 'g', 'att']\n",
    "            subgroups = {\n",
    "                'e':['B', 'A', 'W', 'I'],\n",
    "                'g':['F','M'],\n",
    "                'att': ['black_females', 'black_males', 'asian_females', 'asian_males', 'white_females', 'white_males', 'indian_females', 'indian_males']\n",
    "            }\n",
    "        return sensitive_attributes, subgroups\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A general multi-index table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_table(objective, setting='global'):\n",
    "    \"\"\" Create empty dataframe with correct columns \"\"\"\n",
    "    approaches = ['baseline', 'fsn', 'faircal', 'faircal-gmm']\n",
    "    tuples = []\n",
    "    \n",
    "    if objective == 'fairness_calibration':\n",
    "        indices = {\n",
    "        'rfw' : {\n",
    "            'facenet': ['African', 'Asian', 'Caucasian', 'Indian'],\n",
    "            'facenet-webface': ['African', 'Asian', 'Caucasian', 'Indian'],\n",
    "        },\n",
    "        'bfw' : {\n",
    "            'facenet-webface': ['B', 'A', 'W', 'I', 'F','M', \n",
    "                'black_females', 'black_males', 'asian_females', 'asian_males', 'white_females', 'white_males', 'indian_females', 'indian_males'],\n",
    "            'arcface': ['B', 'A', 'W', 'I', 'F','M', \n",
    "                'black_females', 'black_males', 'asian_females', 'asian_males', 'white_females', 'white_males', 'indian_females', 'indian_males'],\n",
    "            },\n",
    "        }\n",
    "        if setting == 'show_subgroups':\n",
    "            for dataset in indices:\n",
    "                for feature, sens in indices[dataset].items():\n",
    "                    for att in sens:\n",
    "                        for approach in approaches:\n",
    "                            tuples.append((dataset, feature, att, approach))\n",
    "            index = pd.MultiIndex.from_tuples(tuples, names=['dataset', 'feature', 'subgroup', 'approach'])\n",
    "        else:\n",
    "            for dataset in indices:\n",
    "                for feature in indices[dataset]:\n",
    "                    for approach in approaches:\n",
    "                        tuples.append((dataset, feature, approach))\n",
    "            index = pd.MultiIndex.from_tuples(tuples, names=['dataset', 'feature', 'approach'])\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        indices = {\n",
    "        'rfw' : ['facenet', 'facenet-webface'],\n",
    "        'bfw' : ['facenet-webface', 'arcface']\n",
    "        }\n",
    "        for dataset in indices:\n",
    "            for feature in indices[dataset]:\n",
    "                for approach in approaches:\n",
    "                    tuples.append((dataset, feature, approach))\n",
    "        index = pd.MultiIndex.from_tuples(tuples, names=['dataset', 'feature', 'approach'])\n",
    "    \n",
    "    data = pd.DataFrame(index=index)\n",
    "    return approaches, indices, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_for(objective, global_error=1e-2, setting='global'):\n",
    "    \"\"\" Fill dataframe with data and prepare for LaTeX conversion \"\"\"\n",
    "    approaches, indices, data = create_multi_table(objective=objective, setting=setting)\n",
    "    \n",
    "    if objective == 'accuracy':\n",
    "        metrics = ['AUROC', 'TPR @ 0.1% FPR', 'TPR @ 1% FPR']\n",
    "        for metric in metrics:\n",
    "            data[metric] = ''\n",
    "        error = [1e-3, 1e-2]\n",
    "    \n",
    "        for dataset in indices:\n",
    "            for feature in indices[dataset]:\n",
    "                for approach in approaches:\n",
    "                    data_object = DataManager(dataset, feature, approach, objective, fpr_def=error)\n",
    "                    data_work = data_object.provide_data(objective)\n",
    "                    data_work *= 100\n",
    "                    for metric in metrics:\n",
    "                        mean = '%.2f' % data_work[metric].mean()\n",
    "                        std = '%.2f' % data_work[metric].std()\n",
    "                        data.loc[dataset, feature, approach][metric] = f'{str(mean)} ({str(std)})'\n",
    "    \n",
    "    elif objective == 'fairness_calibration':\n",
    "        metrics = ['mean', 'aad', 'mad', 'std']\n",
    "        \n",
    "        if setting == 'show_subgroups':\n",
    "            for metric in metrics:\n",
    "                data[metric] = np.nan\n",
    "            for dataset in indices:\n",
    "                for feature in indices[dataset]:\n",
    "                    for approach in approaches:\n",
    "                        sensitive_attributes = indices[dataset][feature]\n",
    "                        for att in sensitive_attributes:\n",
    "                            data_object = DataManager(dataset, feature, approach, objective)\n",
    "                            data_work = data_object.provide_data(objective)\n",
    "                            data_work = data_work * 100\n",
    "                            for subgroup in data_work.columns:\n",
    "                                loc_select = data.loc[dataset, feature, subgroup, approach]\n",
    "                                group_mean = data_work[subgroup].mean()\n",
    "                                loc_select['mean'] = group_mean\n",
    "                                loc_select['aad'] = np.abs(data_work[subgroup] - group_mean).mean()\n",
    "                                loc_select['mad'] = np.abs(data_work[subgroup] - group_mean).max()\n",
    "                                loc_select['std'] = np.std(data_work[subgroup])\n",
    "        else:\n",
    "            for metric in metrics:\n",
    "                data[metric] = ''\n",
    "            for dataset in indices:\n",
    "                for feature in indices[dataset]:\n",
    "                    for approach in approaches:\n",
    "                        sensitive_attributes = indices[dataset][feature]\n",
    "                        for att in sensitive_attributes:\n",
    "                            data_object = DataManager(dataset, feature, approach, objective)\n",
    "                            data_work = data_object.provide_data(objective)\n",
    "                            data_work = data_work\n",
    "                            \n",
    "                            # computing metrics for averaged over each fold\n",
    "                            mean = data_work.mean(axis=1)\n",
    "                            data_metric = {\n",
    "                                'mean': data_work.mean(axis=1),\n",
    "                                'aad': data_work.sub(mean, axis=0).abs().mean(axis=1),\n",
    "                                'mad': data_work.sub(mean, axis=0).abs().max(axis=1),\n",
    "                                'std': data_work.std(axis=1)  \n",
    "                            }\n",
    "                            \n",
    "                            # computing metrics for averaged over each subgroup\n",
    "                            for metric in metrics:\n",
    "                                mean = '%.2f' % (data_metric[metric].mean() * 100)\n",
    "                                std = '%.2f' % (data_metric[metric].std() * 100)\n",
    "                                data.loc[dataset, feature, approach][metric] = f'{mean} ({std})'\n",
    "                        \n",
    "    elif objective == 'predictive_equality' or objective == 'equal_opportunity':\n",
    "        metrics = ['aad', 'mad', 'std']\n",
    "        for metric in metrics:\n",
    "            data[metric] = ''\n",
    "\n",
    "        for dataset in indices:\n",
    "            att = 'att' if dataset == 'bfw' else 'ethnicity'\n",
    "            _, subgroups = DataManager.get_sensitive_attributes_subgroups(dataset) \n",
    "            for feature in indices[dataset]:\n",
    "                for approach in approaches:\n",
    "                    folds_x_subgroups = pd.DataFrame() \n",
    "                    for i, subgroup in enumerate(subgroups[att]):\n",
    "                        data_object = DataManager(dataset, feature, approach, objective, subgroup=subgroup, at_error=global_error)\n",
    "                        folds_x_subgroups[subgroup] = data_object.provide_data(objective)\n",
    "\n",
    "                    # computing metrics for averaged over each subgroup\n",
    "                    mean = folds_x_subgroups.mean(axis=1)\n",
    "                    data_work = {'aad': folds_x_subgroups.sub(mean, axis=0).abs().mean(axis=1),\n",
    "                                 'mad': folds_x_subgroups.sub(mean, axis=0).abs().max(axis=1),\n",
    "                                 'std': folds_x_subgroups.std(axis=1)\n",
    "                                }\n",
    "                    # computing metrics for averaged over each fold\n",
    "                    for metric in metrics:\n",
    "                        mean = '%.2f' % (data_work[metric].mean() * 100)\n",
    "                        std = '%.2f' % (data_work[metric].std() * 100)\n",
    "                        data.loc[dataset, feature, approach][metric] = f'{mean} ({std})'\n",
    "    else:\n",
    "        print('Please specify a valid objective.')\n",
    "\n",
    "    # Re-structure dataframe to have correct indices and column order\n",
    "    data = data.reset_index()\n",
    "    data = data.pivot(index='approach', columns=['dataset', 'feature']).reorder_levels(['dataset', 'feature', None], axis=1)\n",
    "\n",
    "    good_order = []\n",
    "    for tup in [('rfw', 'facenet'), ('rfw', 'facenet-webface'), ('bfw', 'facenet-webface'), ('bfw', 'arcface')]:\n",
    "        if objective == 'accuracy':\n",
    "            for metric in ['AUROC', 'TPR @ 0.1% FPR', 'TPR @ 1% FPR']: \n",
    "                    good_order.append(tup + (metric,))\n",
    "                    \n",
    "        elif objective == 'fairness_calibration':\n",
    "            for metric in ['mean', 'aad', 'mad', 'std']: \n",
    "                    good_order.append(tup + (metric,))\n",
    "        \n",
    "        elif objective == 'predictive_equality' or objective == 'equal_opportunity':\n",
    "            for metric in ['aad', 'mad', 'std']: \n",
    "                    good_order.append(tup + (metric,))\n",
    "        \n",
    "        else:\n",
    "            print('Please specify a valid objective.')\n",
    "    \n",
    "    data = data[good_order]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_latex_for(df, objective):\n",
    "    \"\"\" Helper function that gets the LaTeX table ready to be printed \"\"\"\n",
    "    latex = df.to_latex()    \n",
    "\n",
    "    if objective == 'accuracy':\n",
    "        latex = latex.replace(' (', '$\\pm$')\n",
    "        latex = latex.replace(')', '')\n",
    "\n",
    "    latex = re.sub('\\((.*?)\\)', '', latex)        \n",
    "\n",
    "    latex = latex.replace('baseline', '& Baseline')\n",
    "    latex = latex.replace('agenda', '& AGENDA')\n",
    "    latex = latex.replace('fsn', '& FSN')\n",
    "    latex = latex.replace('faircal', '& FairCal')\n",
    "    latex = latex.replace('oracle', '& Oracle')\n",
    "    latex = latex.replace('-gmm', '-GMM')\n",
    "    latex = latex.replace('NaN', '--')\n",
    "\n",
    "    return latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_authors_result(df, objective, global_error):\n",
    "    \"\"\" Helper function that adds the author's result for easy reproducibility \"\"\"\n",
    "    authors = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    if objective == 'accuracy':\n",
    "        # Create accuracy table\n",
    "        authors.loc['baseline'] = np.array(['88.26 (0.19)', '18.42 (1.28)', '34.88 (3.27)', \n",
    "                                            '83.95 (0.22)', '11.18 (3.45)', '26.04 (2.11)', \n",
    "                                            '96.06 (0.16)', '33.61 (2.10)', '58.87 (0.92)', \n",
    "                                            '97.41 (0.34)', '86.27 (1.09)', '90.11 (0.87)'])\n",
    "        authors.loc['faircal'] = np.array(['90.58 (0.29)', '23.55 (1.82)', '41.88 (1.99)', \n",
    "                                            '86.71 (0.25)', '20.64 (3.09)', '33.13 (1.67)', \n",
    "                                            '96.90 (0.17)', '46.74 (1.49)', '69.21 (1.19)', \n",
    "                                            '97.44 (0.34)', '86.28 (1.24)', '90.14 (0.86)'])\n",
    "        authors.loc['agenda'] = np.array(['76.83 (0.57)', '8.32 (1.86)', '18.01 (1.44)',\n",
    "                                            '74.51 (0.94)', '6.38 (0.78)', '14.98 (1.11)',\n",
    "                                            '82.42 (0.45)', '15.95 (1.53)', '32.51 (1.24)', \n",
    "                                            '95.09 (0.55)', '69.61 (2.40)', '79.67 (2.06)'])\n",
    "        authors.loc['fsn'] = np.array(['90.05 (0.29)', '23.01 (2.00)', '40.21 (2.09)',\n",
    "                                        '85.84 (0.34)', '17.33 (3.01)', '32.90 (1.03)', \n",
    "                                        '96.77 (0.20)', '47.11 (1.23)', '69.92 (1.01)', \n",
    "                                        '97.35 (0.33)', '86.19 (1.13)', '90.06 (0.84)'])\n",
    "        authors.loc['oracle'] = np.array(['89.74 (0.31)', '21.40 (3.54)', '411.83 (2.98)',\n",
    "                                        '85.23 (0.18)', '16.71 (1.98)', '31.60 (1.08)',\n",
    "                                        '97.28 (0.13)', '45.13 (1.45)', '67.56 (1.05)',\n",
    "                                        '98.91 (0.12)', '86.41 (1.19)', '90.40 (0.91)'])\n",
    "        authors.loc['gmm-discrete'] = np.nan\n",
    "\n",
    "    if objective == 'fairness_calibration':\n",
    "        # Create fairness-calibration table\n",
    "        authors.loc['baseline'] = np.array(['6.37', '2.89', '5.73', '3.77', '5.55', '2.48', '4.97', '2.91', '6.77', '3.63', '5.96', '4.03', '2.57', '1.39', '2.94', '1.63'])\n",
    "        authors.loc['faircal'] = np.array(['1.37', '0.28', '0.50', '0.34', '1.75', '0.41', '0.64', '0.45', '3.09', '1.34', '2.48', '1.55', '2.49', '1.30', '2.68', '1.52'])\n",
    "        authors.loc['agenda'] = np.array(['7.71', '3.11', '6.09', '3.86', '5.71', '2.37', '4.28', '2.85', '13.21', '6.37', '12.91', '7.55', '5.14', '2.48', '5.92', '3.04'])\n",
    "        authors.loc['fsn'] = np.array(['1.43', '0.35', '0.57', '0.40', '2.49', '0.84', '1.19', '0.91', '2.76', '1.38', '2.67', '1.60', '2.65', '1.45', '3.23', '1.71'])\n",
    "        authors.loc['oracle'] = np.array(['1.18', '0.28', '0.53', '0.33', '1.35', '0.38', '0.66', '0.43', '2.23', '1.15', '2.63', '1.40', '1.41', '0.59', '1.30', '0.69'])\n",
    "        authors.loc['gmm-discrete'] = np.nan\n",
    "\n",
    "    if objective == 'predictive_equality' and global_error == 1e-2:\n",
    "        # Create predictive equality table for FPR 1%\n",
    "        authors.loc['baseline'] = np.array(['0.68', '1.02', '0.74', '0.67', '1.23', '0.79', '2.42', '7.48', '3.22', '0.72', '1.51', '0.85'])\n",
    "        authors.loc['faircal'] = np.array(['0.28', '0.46', '0.32', '0.29', '0.57', '0.35', '0.80', '1.79', '0.95', '0.63', '1.46', '0.78'])\n",
    "        authors.loc['agenda'] = np.array(['0.71', '1.14', '0.81', '0.73', '1.08', '0.78', '1.21', '3.09', '1.51', '0.65', '1.78', '0.84'])\n",
    "        authors.loc['fsn'] = np.array(['0.37', '0.68', '0.46', '0.35', '0.61', '0.40', '0.87', '2.19', '1.05', '0.55', '1.27', '0.68'])\n",
    "        authors.loc['oracle'] = np.array(['0.40', '0.69', '0.45', '0.41', '0.74', '0.48', '0.77', '1.71', '0.91', '0.83', '2.08', '1.07'])\n",
    "        authors.loc['gmm-discrete'] = np.nan\n",
    "\n",
    "    if objective == 'predictive_equality' and global_error == 1e-3:\n",
    "        authors.loc['baseline'] = np.array(['0.10', '0.15', '0.10', '0.14', '0.26', '0.16', '0.29', '1.00', '0.40', '0.12', '0.30', '0.15'])\n",
    "        authors.loc['faircal'] = np.array(['0.09', '0.14', '0.10', '0.09', '0.16', '0.10', '0.09', '0.20', '0.11', '0.11', '0.31', '0.15'])\n",
    "        authors.loc['agenda'] = np.array(['0.11', '0.20', '0.13', '0.12', '0.23', '0.14', '0.14', '0.40', '0.18', '0.09', '0.23', '0.11'])\n",
    "        authors.loc['fsn'] = np.array(['0.10', '0.18', '0.11', '0.11', '0.23', '0.23', '0.09', '0.20', '0.11', '0.11', '0.28', '0.14'])\n",
    "        authors.loc['oracle'] =  np.array(['0.11', '0.19', '0.12', '0.11', '0.20', '0.13', '0.12', '0.25', '0.15', '0.12', '0.27', '0.14'])\n",
    "        authors.loc['gmm-discrete'] = np.nan\n",
    "    \n",
    "    # Add Authors/Ours columns\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        new_columns.append(col + ('Authors',))\n",
    "        new_columns.append(col + ('Ours',))\n",
    "\n",
    "    # Copy data\n",
    "    result = pd.DataFrame(index=df.index, columns=pd.MultiIndex.from_tuples(new_columns))\n",
    "    for col in result.columns:\n",
    "        if 'Ours' in col:\n",
    "            result[col] = df[col[:-1]]\n",
    "        if 'Authors' in col:\n",
    "            result[col] = authors[col[:-1]]\n",
    "\n",
    "    # Remove std if objective is not accuracy\n",
    "    if objective != 'accuracy':\n",
    "        for approach in result.index:\n",
    "            result.loc[approach] = result.loc[approach].str.replace('\\((.*?)\\)', '', regex=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Global accuracy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllllllllllllllllll}\n",
      "\\toprule\n",
      "{} & \\multicolumn{12}{l}{rfw} & \\multicolumn{12}{l}{bfw} \\\\\n",
      "{} & \\multicolumn{6}{l}{facenet} & \\multicolumn{6}{l}{facenet-webface} & \\multicolumn{6}{l}{facenet-webface} & \\multicolumn{6}{l}{arcface} \\\\\n",
      "{} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{TPR @ 0.1\\% FPR} & \\multicolumn{2}{l}{TPR @ 1\\% FPR} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{TPR @ 0.1\\% FPR} & \\multicolumn{2}{l}{TPR @ 1\\% FPR} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{TPR @ 0.1\\% FPR} & \\multicolumn{2}{l}{TPR @ 1\\% FPR} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{TPR @ 0.1\\% FPR} & \\multicolumn{2}{l}{TPR @ 1\\% FPR} \\\\\n",
      "{} &       Authors &          Ours &        Authors &          Ours &       Authors &          Ours &         Authors &          Ours &        Authors &          Ours &       Authors &          Ours &         Authors &          Ours &        Authors &          Ours &       Authors &          Ours &       Authors &          Ours &        Authors &          Ours &       Authors &          Ours \\\\\n",
      "approach    &               &               &                &               &               &               &                 &               &                &               &               &               &                 &               &                &               &               &               &               &               &                &               &               &               \\\\\n",
      "\\midrule\n",
      "& Baseline    &  88.26$\\pm$0.19 &  89.97$\\pm$0.58 &   18.42$\\pm$1.28 &  25.27$\\pm$6.51 &  34.88$\\pm$3.27 &  39.92$\\pm$2.40 &    83.95$\\pm$0.22 &  84.46$\\pm$0.47 &   11.18$\\pm$3.45 &  11.14$\\pm$5.34 &  26.04$\\pm$2.11 &  26.45$\\pm$4.90 &    96.06$\\pm$0.16 &  94.62$\\pm$0.17 &   33.61$\\pm$2.10 &  27.93$\\pm$2.02 &  58.87$\\pm$0.92 &  52.79$\\pm$1.74 &  97.41$\\pm$0.34 &  97.34$\\pm$0.36 &   86.27$\\pm$1.09 &  84.75$\\pm$1.26 &  90.11$\\pm$0.87 &  89.51$\\pm$0.98 \\\\\n",
      "& FairCal     &  90.58$\\pm$0.29 &  92.17$\\pm$0.40 &   23.55$\\pm$1.82 &  26.93$\\pm$5.23 &  41.88$\\pm$1.99 &  49.68$\\pm$2.40 &    86.71$\\pm$0.25 &  86.97$\\pm$0.72 &   20.64$\\pm$3.09 &  19.23$\\pm$3.64 &  33.13$\\pm$1.67 &  33.82$\\pm$4.55 &    96.90$\\pm$0.17 &  95.67$\\pm$0.13 &   46.74$\\pm$1.49 &  37.68$\\pm$0.87 &  69.21$\\pm$1.19 &  60.21$\\pm$1.09 &  97.44$\\pm$0.34 &  97.37$\\pm$0.35 &   86.28$\\pm$1.24 &  84.95$\\pm$1.32 &  90.14$\\pm$0.86 &  89.55$\\pm$1.01 \\\\\n",
      "& FairCal-GMM &           -- &  92.46$\\pm$0.43 &            -- &  29.88$\\pm$4.34 &           -- &  50.86$\\pm$3.42 &             -- &  87.19$\\pm$0.53 &            -- &  20.49$\\pm$5.36 &           -- &  33.44$\\pm$4.09 &             -- &  95.48$\\pm$0.15 &            -- &  35.39$\\pm$1.46 &           -- &  58.49$\\pm$1.57 &           -- &  97.35$\\pm$0.37 &            -- &  84.78$\\pm$1.21 &           -- &  89.51$\\pm$1.00 \\\\\n",
      "& FSN         &  90.05$\\pm$0.29 &  91.30$\\pm$0.35 &   23.01$\\pm$2.00 &  26.79$\\pm$4.63 &  40.21$\\pm$2.09 &  44.52$\\pm$2.91 &    85.84$\\pm$0.34 &  86.24$\\pm$0.63 &   17.33$\\pm$3.01 &  17.98$\\pm$5.74 &  32.90$\\pm$1.03 &  31.68$\\pm$2.02 &    96.77$\\pm$0.20 &  94.84$\\pm$0.22 &   47.11$\\pm$1.23 &  37.87$\\pm$0.98 &  69.92$\\pm$1.01 &  59.86$\\pm$1.23 &  97.35$\\pm$0.33 &  97.32$\\pm$0.35 &   86.19$\\pm$1.13 &  84.77$\\pm$1.20 &  90.06$\\pm$0.84 &  89.49$\\pm$0.98 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marga\\AppData\\Local\\Temp\\ipykernel_24620\\3072526165.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex = df.to_latex()\n"
     ]
    }
   ],
   "source": [
    "accuracy_table = get_table_for('accuracy')\n",
    "accuracy_table = add_authors_result(accuracy_table, objective='accuracy', global_error=1e-2)\n",
    "accuracy_in_latex = get_latex_for(accuracy_table, 'accuracy')\n",
    "print(accuracy_in_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fairness Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllll}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{mean} & \\multicolumn{2}{l}{aad} & \\multicolumn{2}{l}{mad} & \\multicolumn{2}{l}{std} \\\\\n",
      "{} & Authors &   Ours & Authors &   Ours & Authors &   Ours & Authors &   Ours \\\\\n",
      "approach    &         &        &         &        &         &        &         &        \\\\\n",
      "\\midrule\n",
      "& Baseline    &    5.55 &  7.08  &    2.48 &  2.66  &    4.97 &  4.93  &    2.91 &  3.66  \\\\\n",
      "& FairCal     &    1.75 &  3.76  &    0.41 &  0.79  &    0.64 &  1.40  &    0.45 &  1.06  \\\\\n",
      "& FairCal-GMM &     -- &  3.67  &     -- &  0.55  &     -- &  1.07  &     -- &  0.78  \\\\\n",
      "& FSN         &    2.49 &  3.90  &    0.84 &  0.54  &    1.19 &  1.05  &    0.91 &  0.75  \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marga\\AppData\\Local\\Temp\\ipykernel_19868\\2907280951.py:3: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  fairness_cal_table = fairness_cal_table['rfw', 'facenet-webface']\n",
      "C:\\Users\\marga\\AppData\\Local\\Temp\\ipykernel_19868\\3072526165.py:4: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex = df.to_latex()\n"
     ]
    }
   ],
   "source": [
    "fairness_cal_table = get_table_for(objective='fairness_calibration', setting='global')\n",
    "fairness_cal_table = add_authors_result(fairness_cal_table, 'fairness_calibration', global_error=1e-2)\n",
    "fairness_cal_table = fairness_cal_table['rfw', 'facenet-webface']\n",
    "fairness_cal_in_latex = get_latex_for(fairness_cal_table, 'fairness_calibration')\n",
    "print(fairness_cal_in_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Predictive Equality ('fpr at fpr') and 4. Equal Opportunity ('fnr at fnr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectives = ['predictive_equality', 'equal_opportunity']\n",
    "global_errors = [1e-3, 1e-2]\n",
    "\n",
    "\n",
    "objectives = ['equal_opportunity']\n",
    "for objective in objectives:\n",
    "    for global_error in global_errors:\n",
    "        current_df = get_table_for(objective, global_error=global_error)\n",
    "        print(f'\\n{objective} at a global error rate = {global_error * 100}%')                \n",
    "        display(HTML(current_df.to_html()))\n",
    "\n",
    "eq_opp_in_latex = get_latex_for(current_df, 'equal_opportunity')\n",
    "print(eq_opp_in_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising example results for predictive equality and equal opportunity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example conversion from dataframe to latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = current_df.to_latex()\n",
    "print(txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "38bfd7062c2e40c254718556666e4422f04be01044787720715af94b520c2c5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
