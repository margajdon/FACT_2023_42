{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blind-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "equivalent-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_approaches = {\n",
    "    'naive':'Naive',\n",
    "    'fsn':'Fair Score',\n",
    "    'bcm':'FairCal (Ours)',\n",
    "    'oracle':'Oracle (Ours)'}\n",
    "title_calibration_methods = {\n",
    "    'binning': 'Binning',\n",
    "    'splines': 'Splines',\n",
    "    'isotonic_regression': 'Isotonic Regression',\n",
    "    'beta': 'Beta Calibration'\n",
    "}\n",
    "title_features = {\n",
    "    'facenet':'FaceNet (VGGFace2)',\n",
    "    'facenet-webface':'FaceNet (Webface)',\n",
    "    'arcface': 'ArcFace'}\n",
    "title_metrics = {\n",
    "    'mean': 'Mean',\n",
    "    'aad': 'AAD',\n",
    "    'mad': 'MAD',\n",
    "    'std': 'STD'}\n",
    "caption_metrics = {\n",
    "     'mean': 'Mean',\n",
    "     'aad': 'AAD (Average Absolute Deviation)',\n",
    "     'mad': 'MAD (Maximum Absolute Deviation)',\n",
    "     'std': 'STD (Standard Deviation)',\n",
    "}\n",
    "title_keys = {\n",
    "    'baseline': 'Baseline',\n",
    "    'agenda': 'AGENDA',\n",
    "    'ftc': 'FTC',\n",
    "    'fct2': 'FCT',\n",
    "    'fsn': 'FSN',\n",
    "    'faircal': 'FairCal (Ours)',\n",
    "    'bmc_imp': 'BMC Improved (Ours)',\n",
    "    'bmc_error': 'BMC Error (Ours)',\n",
    "    'bmc_min': 'BMC Min (Ours)',\n",
    "    'bmc_proj': 'BMC Proj (Ours)',\n",
    "    'oracle': 'Oracle (Ours)'}\n",
    "header_titles = {\n",
    "    'African': 'Af',\n",
    "    'Asian': 'As',\n",
    "    'Caucasian': 'Ca',\n",
    "    'Indian': 'In',\n",
    "    'asian_females': 'AsF',\n",
    "    'asian_males': 'AsM',\n",
    "    'black_females': 'AfF',\n",
    "    'black_males': 'AfM',\n",
    "    'indian_females': 'IF',\n",
    "    'indian_males': 'IM',\n",
    "    'white_females': 'CF',\n",
    "    'white_males': 'CM',\n",
    "    'Global': 'Gl',\n",
    "    'B': 'Af',\n",
    "    'A': 'As',\n",
    "    'W': 'C',\n",
    "    'I': 'I',\n",
    "    'F': 'F',\n",
    "    'M': 'M'\n",
    "}\n",
    "title_datasets = {\n",
    "    'rfw': 'RFW',\n",
    "    'bfw': 'BFW',\n",
    "    'bfw_eq': 'BFW (equal)'\n",
    "}\n",
    "caption_calibration_methods = {\n",
    "    'binning': 'histogram binning',\n",
    "    'splines': 'spline calibration',\n",
    "    'isotonic_regression': 'isotonic regression',\n",
    "    'beta': 'beta calibration'\n",
    "}\n",
    "caption_measures = {\n",
    "    'ece': 'ECE',\n",
    "    'ks': 'KS',\n",
    "    'brier': 'Brier Score'\n",
    "}\n",
    "features_datasets = {\n",
    "    'rfw': ['facenet', 'facenet-webface'],\n",
    "    'bfw': ['facenet-webface'],\n",
    "    'bfw_eq': ['facenet-webface', 'arcface']\n",
    "}\n",
    "attributes_datasets = {\n",
    "    'rfw': 'ethnicity',\n",
    "    'bfw': 'att',\n",
    "    'bfw_eq': 'att'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacterial-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_fontsize = 16\n",
    "title_fontsize = 20\n",
    "label_fontsize = 18\n",
    "ticks_fontsize = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stone-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['facenet', 'facenet-webface']\n",
    "keys = ['baseline', 'agenda', 'ftc', 'fsn','faircal', 'oracle'] # also have results for bmc_imp and bmc_error\n",
    "datasets = ['rfw', 'bfw']\n",
    "measures = ['ece', 'ks', 'brier']\n",
    "metrics = ['mean', 'aad', 'mad', 'std']\n",
    "folds = [1,2,3,4,5]\n",
    "nbins = {'rfw':10, 'bfw':25}\n",
    "calibration_methods = ['beta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "prospective-bernard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory ../tables_iclr failed\n",
      "Creation of the directory ../tables_iclr/fair_calibration failed\n",
      "Creation of the directory ../tables_iclr/subgroup_calibration failed\n",
      "Creation of the directory ../tables_iclr/predictive_equality_at_fpr failed\n",
      "Creation of the directory ../tables_iclr/equal_opportunity_at_fnr failed\n",
      "Creation of the directory ../tables_iclr/global_performance failed\n"
     ]
    }
   ],
   "source": [
    "# define the name of the directory to be created\n",
    "path = \"../tables_iclr\"\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)\n",
    "for folder_name in ['fair_calibration', 'subgroup_calibration', 'predictive_equality_at_fpr', 'equal_opportunity_at_fnr', 'global_performance']:\n",
    "    path = \"../tables_iclr/\"+folder_name\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "expected-evolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of the directory ../figs_iclr failed\n",
      "Creation of the directory ../figs_iclr/fair_calibration failed\n",
      "Creation of the directory ../figs_iclr/robustness failed\n"
     ]
    }
   ],
   "source": [
    "# define the name of the directory to be created\n",
    "path = \"../figs_iclr\"\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)\n",
    "for folder_name in ['fair_calibration', 'robustness']:\n",
    "    path = \"../figs_iclr/\"+folder_name\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (\"Creation of the directory %s failed\" % path)\n",
    "    else:\n",
    "        print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-rabbit",
   "metadata": {},
   "source": [
    "# Load Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "treated-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_measures(dataset,feature,approach,subgroups,att,measure,calibration_method,nbins,n_clusters):\n",
    "    if 'faircal' in approach:\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'faircal':        \n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'fsn':\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}_fpr_1e-03.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'baseline':\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'oracle':\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "#     elif approach == 'ftc':\n",
    "#         temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "#     elif approach == 'agenda':\n",
    "#         temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()    \n",
    "    else:\n",
    "        print('hello')\n",
    "        temp = np.load(f\"experiments/{dataset}_{feature}_{approach}_{calibration_method}_nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "    data = np.zeros((5,len(subgroups[att])))\n",
    "    for fold in range(1,6):\n",
    "        for j, subgroup in enumerate(subgroups[att]):\n",
    "            data[fold-1,j] = temp['fold'+str(fold)][measure][att][subgroup]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "secondary-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensitive_attributes_subgroups(dataset):\n",
    "    if dataset == 'rfw':\n",
    "        sensitive_attributes = ['ethnicity']\n",
    "        subgroups = {'ethnicity':['African', 'Asian', 'Caucasian', 'Indian']}\n",
    "    elif 'bfw' in dataset:\n",
    "        sensitive_attributes = ['e', 'g', 'att']\n",
    "        subgroups = {\n",
    "            'e':['B', 'A', 'W', 'I'],\n",
    "            'g':['F','M'],\n",
    "            'att': ['black_females', 'black_males', 'asian_females', 'asian_males', 'white_females', 'white_males', 'indian_females', 'indian_males']\n",
    "        }\n",
    "    return sensitive_attributes, subgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "injured-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks = np.array([5,10,15,20,25,50,75,100])\n",
    "ks = np.array([100])\n",
    "folds = [1,2,3,4,5]\n",
    "# datasets = ['rfw', 'bfw']\n",
    "datasets = ['bfw']\n",
    "features = ['facenet-webface']\n",
    "keys = ['faircal'] # also have results for bmc_imp and bmc_error\n",
    "# nbins = {'bfw': 100}\n",
    "\n",
    "mean = {}\n",
    "aad = {}\n",
    "mad = {}\n",
    "std = {}\n",
    "for measure in measures:\n",
    "    mean[measure] = {}\n",
    "    aad[measure] = {}\n",
    "    mad[measure] = {}\n",
    "    std[measure] = {}\n",
    "    for calibration in calibration_methods:\n",
    "        mean[measure][calibration] = {}\n",
    "        aad[measure][calibration] = {}\n",
    "        mad[measure][calibration] = {}\n",
    "        std[measure][calibration] = {}\n",
    "        for dataset in datasets:\n",
    "            mean[measure][calibration][dataset] = {}\n",
    "            aad[measure][calibration][dataset] = {}\n",
    "            mad[measure][calibration][dataset] = {}\n",
    "            std[measure][calibration][dataset] = {}\n",
    "            sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "            for feature in features:\n",
    "                mean[measure][calibration][dataset][feature] = {}\n",
    "                aad[measure][calibration][dataset][feature] = {}\n",
    "                mad[measure][calibration][dataset][feature] = {}\n",
    "                std[measure][calibration][dataset][feature] = {}\n",
    "                for att in sensitive_attributes:\n",
    "                    mean[measure][calibration][dataset][feature][att] = {}\n",
    "                    aad[measure][calibration][dataset][feature][att] = {}\n",
    "                    mad[measure][calibration][dataset][feature][att] = {}\n",
    "                    std[measure][calibration][dataset][feature][att] = {}\n",
    "                    for key in keys:\n",
    "                        mean[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        aad[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        mad[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        std[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        data_work = np.zeros((len(folds),len(subgroups[att])))\n",
    "                        for i_k, k in enumerate(ks):\n",
    "                            data_work = load_measures(dataset,feature,key,subgroups,att,measure,calibration,nbins[dataset],k)\n",
    "                            if key != 'brier':\n",
    "                                data_work *= 100\n",
    "                            mean_aux = data_work.mean(axis=1).reshape(-1,1)\n",
    "                            mean[measure][calibration][dataset][feature][att][key][:,i_k] = data_work.mean(axis=1)\n",
    "                            aad[measure][calibration][dataset][feature][att][key][:,i_k] = np.abs(data_work-mean_aux).mean(axis=1)\n",
    "                            mad[measure][calibration][dataset][feature][att][key][:,i_k] = np.abs(data_work-mean_aux).max(axis=1)\n",
    "                            std[measure][calibration][dataset][feature][att][key][:,i_k] = np.std(data_work,axis=1)\n",
    "data = {'mean':mean, 'aad':aad, 'mad': mad, 'std': std}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-manufacturer",
   "metadata": {},
   "source": [
    "# Creating Figures - Fair-Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "flexible-target",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m data[metric][measure][calibration][dataset][feature][att][key]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m y_std \u001b[38;5;241m=\u001b[39m data[metric][measure][calibration][dataset][feature][att][key]\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m \u001b[43max\u001b[49m\u001b[43m[\u001b[49m\u001b[43mix\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(ks)),y,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m ax[ix]\u001b[38;5;241m.\u001b[39mfill_between(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(ks)),y\u001b[38;5;241m-\u001b[39my_std,y\u001b[38;5;241m+\u001b[39my_std,alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     20\u001b[0m legend\u001b[38;5;241m.\u001b[39mappend(title_keys[key])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AxesSubplot' object is not subscriptable"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for measure in measures:\n",
    "    for calibration in calibration_methods:\n",
    "        for metric in metrics:\n",
    "            total_graphs = 0\n",
    "            for dataset in datasets:\n",
    "                total_graphs += len(features_datasets[dataset])\n",
    "            fig, ax = plt.subplots(1,total_graphs, figsize=(total_graphs*5,5))\n",
    "            \n",
    "            ix = 0\n",
    "            for dataset in datasets:\n",
    "                sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "                att = attributes_datasets[dataset]\n",
    "                for feature in features_datasets[dataset]:\n",
    "                    legend = []\n",
    "                    for key in keys:\n",
    "                        y = data[metric][measure][calibration][dataset][feature][att][key].mean(axis=0)\n",
    "                        y_std = data[metric][measure][calibration][dataset][feature][att][key].std(axis=0)\n",
    "                        ax[ix].plot(np.arange(len(ks)),y,marker='o')\n",
    "                        ax[ix].fill_between(np.arange(len(ks)),y-y_std,y+y_std,alpha = 0.5)\n",
    "                        legend.append(title_keys[key])\n",
    "                    ax[ix].legend(legend)\n",
    "                    ax[ix].xaxis.set_ticks(np.arange(len(ks)))\n",
    "                    ax[ix].xaxis.set_ticklabels(ks)\n",
    "                    ax[ix].set_title(f\"{title_datasets[dataset]} - {title_features[feature]}\\n {caption_measures[measure]} - {title_metrics[metric]}\")\n",
    "                    ix += 1\n",
    "\n",
    "            \n",
    "        save_as = f\"{calibration}_{dataset}_{att}_{measure}_{metric}.pdf\"\n",
    "#         plt.savefig('../figs_neurips/fair_calibration/'+save_as, bbox_inches='tight')\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-addiction",
   "metadata": {},
   "source": [
    "# Creating Tables - Fair-Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-singing",
   "metadata": {},
   "source": [
    "## Bias Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "acknowledged-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_bias_measures(calibration, nbins, measure):\n",
    "    header_features = [title_features[feature] for feature in features]\n",
    "    k = 100\n",
    "    i_k = -1\n",
    "    \n",
    "    # Setup Row Headers\n",
    "    row_headers = []\n",
    "    for key in keys:\n",
    "        row_headers.append(title_keys[key])\n",
    "\n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    col_index_lvl_3 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            for metric in metrics:\n",
    "                col_index_lvl_1.append(title_datasets[dataset])\n",
    "                col_index_lvl_2.append(title_features[feature])\n",
    "                col_index_lvl_3.append(title_metrics[metric])\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        att = attributes_datasets[dataset]\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for i3, metric in enumerate(metrics):\n",
    "                for row_i, key in enumerate(keys):\n",
    "                    data_work = data[metric][measure][calibration][dataset][feature][att][key][:,i_k].mean()\n",
    "                    column_j = len(features_datasets[dataset])*len(metrics)*i1+len(metrics)*i2\n",
    "                    table[row_i,column_j+i3] = data_work\n",
    "    db_temp = pd.DataFrame(\n",
    "            table,\n",
    "            index=row_headers,\n",
    "            columns=col_headers)\n",
    "    print(db_temp)\n",
    "    \n",
    "#     print(table[0])\n",
    "#     print(len(keys)-1)\n",
    "#     print(table[:(len(keys)-1),:])\n",
    "    \n",
    "    txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "    # Bold entries\n",
    "    txt = txt.split('\\n')\n",
    "    bold_idx = [table[0].argmin(axis=0)]\n",
    "#     for i, idx in enumerate(bold_idx):\n",
    "#         temp = txt[6+idx].split(' & ')\n",
    "#         print(temp)\n",
    "#         if '\\\\\\\\' in temp[i+1]:\n",
    "#             temp[i+1] = '\\\\textbf{'+str(float(temp[i+1][:-2]))+'}\\\\\\\\'\n",
    "#         else:\n",
    "#             temp[i+1] = '\\\\textbf{'+str(float(temp[i+1]))+'}'\n",
    "\n",
    "#         txt[6+idx] = ' & '.join(temp)\n",
    "    txt = '\\n'.join(txt)\n",
    "    txt = txt.replace('{}','',2)\n",
    "    txt = txt.replace('{}','\\hfill $(\\downarrow)$')\n",
    "    txt = txt.replace('{tabular}{c}','{tabular}{l|cccc|cccc|cccc|cccc}')\n",
    "    txt = txt.replace('multicolumn{8}{c}','multicolumn{8}{c|}',1)\n",
    "    txt = txt.replace('multicolumn{4}{c}','multicolumn{4}{c|}',2)\n",
    "    txt = txt.replace('multicolumn{4}{c|}','multicolumn{4}{c}',1)\n",
    "    txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "    txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "    txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "    txt = txt.replace('\\\\toprule\\n','')\n",
    "    txt = txt.replace('\\\\bottomrule\\n','')\n",
    "    txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "    # Italic entries for Oracle method\n",
    "    txt = txt.split('\\n')\n",
    "    for i in range(table.shape[1]):\n",
    "        temp = txt[4+len(keys)].split(' & ')\n",
    "        if '\\\\\\\\' in temp[i]:\n",
    "            temp[i] = '\\\\textit{'+str(float(temp[i][:-2]))+'}\\\\\\\\'\n",
    "        elif i == 0:\n",
    "            temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "        else:\n",
    "            temp[i] = '\\\\textit{'+str(float(temp[i]))+'}'\n",
    "        txt[4+len(keys)] = ' & '.join(temp)\n",
    "    txt = '\\n'.join(txt)\n",
    "    txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "    label = f\"table:fair_calibration_{measure}\"\n",
    "    \n",
    "    caption = \"\\\\textbf{Fairness calibration:} measured by the mean \"+caption_measures[measure]+\"across the sensitive subgroups. \\textbf{Bias:} measured by the deviations of \"+caption_measures[measure]+\" across subgroups in terms of three deviation measures: Average Absolute Deviation (AAD), Maximum Absolute Deviation (MAD), and Standard Deviation (STD) (lower is better).\"\n",
    "    txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "    with open(f\"../tables_iclr/fair_calibration/{calibration}_nbins{nbins[dataset]}_{measure}.tex\",'w') as file:\n",
    "        file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "motivated-medicaid",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             BFW                              \n",
      "               FaceNet (Webface)                              \n",
      "                            Mean       AAD       MAD       STD\n",
      "FairCal (Ours)          25.88128  2.225731  5.856818  3.047468\n",
      "                             BFW                              \n",
      "               FaceNet (Webface)                              \n",
      "                            Mean       AAD       MAD       STD\n",
      "FairCal (Ours)         25.306022  1.755815  4.741518  2.403994\n",
      "                             BFW                              \n",
      "               FaceNet (Webface)                              \n",
      "                            Mean       AAD       MAD       STD\n",
      "FairCal (Ours)         30.326272  1.725456  3.843528  2.042016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/ywvr68ys7ln1r7pltrqvxv4w0000gn/T/ipykernel_2087/3938479675.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
      "/var/folders/ns/ywvr68ys7ln1r7pltrqvxv4w0000gn/T/ipykernel_2087/3938479675.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
      "/var/folders/ns/ywvr68ys7ln1r7pltrqvxv4w0000gn/T/ipykernel_2087/3938479675.py:44: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n"
     ]
    }
   ],
   "source": [
    "for calibration in calibration_methods:\n",
    "    for measure in measures:\n",
    "        create_table_bias_measures(calibration, nbins, measure)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "explicit-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_bias_measures_std(calibration, nbins, measure):\n",
    "    header_features = [title_features[feature] for feature in features]\n",
    "    k = 100\n",
    "    i_k = -1\n",
    "    \n",
    "    # Setup Row Headers\n",
    "    row_headers = []\n",
    "    for key in keys:\n",
    "        row_headers.append(title_keys[key])\n",
    "\n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    col_index_lvl_3 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            for metric in metrics:\n",
    "                col_index_lvl_1.append(title_datasets[dataset])\n",
    "                col_index_lvl_2.append(title_features[feature])\n",
    "                col_index_lvl_3.append(title_metrics[metric])\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "    table_std = np.zeros((len(row_headers),len(col_headers)))\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        att = attributes_datasets[dataset]\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for i3, metric in enumerate(metrics):\n",
    "                for row_i, key in enumerate(keys):\n",
    "                    data_work = data[metric][measure][calibration][dataset][feature][att][key][:,i_k].mean()\n",
    "                    data_work_std = data[metric][measure][calibration][dataset][feature][att][key][:,i_k].std()\n",
    "                    column_j = len(features_datasets[dataset])*len(metrics)*i1+len(metrics)*i2\n",
    "                    table[row_i,column_j+i3] = data_work\n",
    "                    table_std[row_i,column_j+i3] = data_work_std\n",
    "\n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            col_index_lvl_1.append(title_datasets[dataset])\n",
    "            col_index_lvl_2.append(title_features[feature])\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    for i_metric, metric in enumerate(metrics):\n",
    "        aux1 = np.apply_along_axis(lambda y: ['%1.2f' % i for i in y], 0, table[:,i_metric::len(metrics)])\n",
    "        aux2 = np.apply_along_axis(lambda y: ['@ %1.2f' % i for i in y], 0, table_std[:,i_metric::len(metrics)])\n",
    "        table_new = np.core.defchararray.add(aux1, aux2)\n",
    "        db_temp = pd.DataFrame(\n",
    "                table_new,\n",
    "                index=row_headers,\n",
    "                columns=col_headers)            \n",
    "        txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "        txt = txt.replace('@','$\\pm$')\n",
    "        # Bold entries\n",
    "        txt = txt.split('\\n')\n",
    "        bold_idx = table[:,i_metric::len(metrics)][:(len(keys)-1),:].argmin(axis=0)\n",
    "        for i, idx in enumerate(bold_idx):\n",
    "            temp = txt[5+idx].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i+1]:\n",
    "                temp[i+1] = '\\\\textbf{'+temp[i+1][:-2]+'}\\\\\\\\'\n",
    "            else:\n",
    "                temp[i+1] = '\\\\textbf{'+temp[i+1]+'}'\n",
    "\n",
    "            txt[5+idx] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        txt = txt.replace('{tabular}{c}','{tabular}{l|cc|cc}')\n",
    "        txt = txt.replace('multicolumn{2}{c}','multicolumn{2}{c|}',1)\n",
    "        txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "        txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "        txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "        txt = txt.replace('\\\\toprule\\n','')\n",
    "        txt = txt.replace('\\\\bottomrule\\n','')\n",
    "        txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "        # Italic entries for Oracle method\n",
    "        txt = txt.split('\\n')\n",
    "        for i in range(table_new.shape[1]):\n",
    "            temp = txt[3+len(keys)].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i]:\n",
    "                temp[i] = '\\\\textit{'+temp[i][:-2]+'}\\\\\\\\'\n",
    "            elif i == 0:\n",
    "                temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "            else:\n",
    "                temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "            txt[3+len(keys)] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "        label = f\"table:fair_calibration_{measure}_{metric}\"\n",
    "        if metric == 'mean':\n",
    "            caption = \"Fairness Calibration as measured by \"+caption_metrics[metric]+\" across sensitive subgroups.\"\n",
    "        else:\n",
    "            caption = \"Bias in fairness-calibration as measured by the deviations of \"+caption_measures[measure]+\" across subgroups in terms of \"+caption_metrics[metric]+'.'\n",
    "        txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\\\centering\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt  \n",
    "        with open(f\"../tables_iclr/fair_calibration/{calibration}_nbins{nbins[dataset]}_{measure}_{metric}.tex\",'w') as file:\n",
    "            file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "exempt-banana",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/ywvr68ys7ln1r7pltrqvxv4w0000gn/T/ipykernel_2087/2232595357.py:57: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmin of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m calibration \u001b[38;5;129;01min\u001b[39;00m calibration_methods:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m measure \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mks\u001b[39m\u001b[38;5;124m'\u001b[39m]:\u001b[38;5;66;03m#measures:\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mcreate_table_bias_measures_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalibration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeasure\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 61\u001b[0m, in \u001b[0;36mcreate_table_bias_measures_std\u001b[0;34m(calibration, nbins, measure)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Bold entries\u001b[39;00m\n\u001b[1;32m     60\u001b[0m txt \u001b[38;5;241m=\u001b[39m txt\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m bold_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi_metric\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(bold_idx):\n\u001b[1;32m     63\u001b[0m     temp \u001b[38;5;241m=\u001b[39m txt[\u001b[38;5;241m5\u001b[39m\u001b[38;5;241m+\u001b[39midx]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m & \u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmin of an empty sequence"
     ]
    }
   ],
   "source": [
    "for calibration in calibration_methods:\n",
    "    for measure in ['ks']:#measures:\n",
    "        create_table_bias_measures_std(calibration, nbins, measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-victory",
   "metadata": {},
   "source": [
    "## Calibration per subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-trustee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_calibration_subgroup(calibration, nbins, dataset, measure):\n",
    "    header_features = [title_features[feature] for feature in features]\n",
    "    k = 100 \n",
    "    sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "    for att in sensitive_attributes:\n",
    "        headers = []\n",
    "        subgroups[att].insert(0,'Global')\n",
    "        for subgroup in subgroups[att]:\n",
    "            if dataset == 'rfw':\n",
    "                headers.append(subgroup)\n",
    "            else:\n",
    "                headers.append(header_titles[subgroup])\n",
    "        index_lvl_1 = []\n",
    "        index_lvl_2 = []\n",
    "        for i, feature in enumerate(features_datasets[dataset]):\n",
    "            for j, subgroup in enumerate(subgroups[att]):\n",
    "                index_lvl_1.append(title_features[feature])\n",
    "                index_lvl_2.append(header_titles[subgroup])\n",
    "        table = np.zeros((len(keys),len(features_datasets[dataset])*len(subgroups[att])))\n",
    "        for i, feature in enumerate(features_datasets[dataset]):\n",
    "            for j, key in enumerate(keys):\n",
    "                data_work = load_measures(dataset,feature,key,subgroups,att,measure,calibration,nbins[dataset],k)\n",
    "                if measure != 'brier':\n",
    "                    data_work *=100\n",
    "                table[j,len(subgroups[att])*i:len(subgroups[att])*i+len(headers)] = data_work.mean(axis=0)\n",
    "\n",
    "        arrays = [index_lvl_1,index_lvl_2]\n",
    "        tuples = list(zip(*arrays))\n",
    "        db_temp = pd.DataFrame(\n",
    "                table,\n",
    "                index=[title_keys[key] for key in keys],\n",
    "                columns = pd.MultiIndex.from_tuples(tuples))\n",
    "        txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "        txt = txt.replace('{}','',1)\n",
    "        txt = txt.replace('{}','\\hfill $(\\downarrow)$')\n",
    "        txt = txt.replace('{tabular}{c}','{tabular}{l'+('|'+('c@{\\hskip 4pt}'*len(subgroups[att])))*len(features_datasets[dataset])+'}')\n",
    "        txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "        txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "        txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "        txt = txt.replace('\\\\toprule\\n','')\n",
    "        txt = txt.replace('\\\\bottomrule\\n','')\n",
    "#         i_min = 0\n",
    "#         i_max = 4\n",
    "#         start = 4\n",
    "#         txt = txt.split('\\n')\n",
    "#         bold_idx = table[i_min:i_max,:].argmin(axis=0)\n",
    "#         for i, idx in enumerate(bold_idx):\n",
    "#             temp = txt[start+idx].split(' & ')\n",
    "#             if '\\\\\\\\' in temp[i+1]:\n",
    "#                 temp[i+1] = '\\\\textbf{'+str(float(temp[i+1][:-2]))+'}\\\\\\\\'\n",
    "#             else:\n",
    "#                 temp[i+1] = '\\\\textbf{'+str(float(temp[i+1]))+'}'\n",
    "#             txt[start+idx] = ' & '.join(temp)\n",
    "#         txt = '\\n'.join(txt)\n",
    "        if dataset == 'rfw':\n",
    "            txt = txt.replace('multicolumn{5}{c}','multicolumn{5}{c|}',1)\n",
    "        else:\n",
    "            if att == 'att':\n",
    "                txt = txt.replace('multicolumn{9}{c}','multicolumn{9}{c|}',1)\n",
    "            elif att == 'e':\n",
    "                txt = txt.replace('multicolumn{5}{c}','multicolumn{5}{c|}',1)\n",
    "            elif att == 'g':\n",
    "                txt = txt.replace('multicolumn{3}{c}','multicolumn{3}{c|}',1)\n",
    "        txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "        # Italic entries for Oracle method\n",
    "        txt = txt.split('\\n')\n",
    "        for i in range(table.shape[1]):\n",
    "            temp = txt[3+len(keys)].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i]:\n",
    "                temp[i] = '\\\\textit{'+str(float(temp[i][:-2]))+'}\\\\\\\\'\n",
    "            elif i == 0:\n",
    "                temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "            else:\n",
    "                temp[i] = '\\\\textit{'+str(float(temp[i]))+'}'\n",
    "            txt[3+len(keys)] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        label = f\"table:subgroup_calibration_{dataset}_{measure}_{att}\"\n",
    "        if dataset == 'rfw':\n",
    "            caption = f'{caption_measures[measure]} on all the pairs (Global (Gl)) and on each ethnicity subgroup (African (Af), Asian (As), Caucasian (Ca), Indian (In) using {caption_calibration_methods[calibration]} on the RFW dataset.'\n",
    "            txt = txt.replace('end{tabular}','end{tabular}\\n\\\\end{table}')\n",
    "            txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\centering\\n\"+txt\n",
    "        elif 'bfw' in dataset:\n",
    "            txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "            if att == 'att':\n",
    "                caption = f'{caption_measures[measure]} on all the pairs (Global (Gl)) and on each ethnicity and gender subgroup (African Females (AfF), African Males (AfM), Asian Females (AsF), Asian Males (AsM), Caucasian Females (CF), Caucasian Males (CM), Indian Females (IF), Indian Males (IM)) using {caption_calibration_methods[calibration]} on the BFW dataset.'\n",
    "            elif att == 'e':\n",
    "                caption = f'{caption_measures[measure]} on all the pairs (Global (Gl)) and on each ethnicity subgroup (African (Af), Asian (As), Caucasian (Ca), Indian (In)) using {caption_calibration_methods[calibration]} on the BFW dataset.'\n",
    "            elif att == 'g':\n",
    "                caption = f'{caption_measures[measure]} on all the pairs (Global (Gl)) and on each gender subgroup (Females (F), Males (M)) using {caption_calibration_methods[calibration]} on the BFW dataset.'\n",
    "            txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\centering\\n\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "        with open(f\"../tables_iclr/subgroup_calibration/{calibration}_{dataset}_{measure}_{att}.tex\",'w') as file:\n",
    "            file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "for calibration in calibration_methods:\n",
    "    for dataset_name in ['bfw']: #omit rfw\n",
    "        \n",
    "        for measure in measures:\n",
    "            create_table_calibration_subgroup(calibration, nbins, dataset_name, measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-wedding",
   "metadata": {},
   "source": [
    "# Global Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "turkish-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_stats(calibration_method, nbins, dataset,feature,approach,att,n_clusters,fpr_def):\n",
    "    if 'faircal' in approach:\n",
    "        temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'_nclusters_'+str(n_clusters)+'.npy',allow_pickle=True).item()\n",
    "        key = 'calibration'\n",
    "    elif approach == 'fsn':\n",
    "        temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'_nclusters_'+str(n_clusters)+'_fpr_1e-03.npy',allow_pickle=True).item()\n",
    "        key = 'pre_calibration'\n",
    "    else:\n",
    "        temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'.npy',allow_pickle=True).item()\n",
    "        if approach == 'oracle':\n",
    "            key = 'calibration'\n",
    "        else:\n",
    "            key = 'pre_calibration'\n",
    "    data = np.zeros((5,1+len(fpr_def)))\n",
    "    for fold in range(1,6):\n",
    "        fpr = temp['fold'+str(fold)]['fpr'][att]['Global'][key]\n",
    "        tpr = temp['fold'+str(fold)]['tpr'][att]['Global'][key]\n",
    "        data[fold-1,0] = sklearn.metrics.auc(fpr,tpr)\n",
    "        data[fold-1,1:] = np.interp(fpr_def, fpr, tpr)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "flying-implementation",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['baseline', 'fsn', 'faircal', 'oracle'] # change back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "absolute-brief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             BFW                    \n",
      "               FaceNet (Webface)                    \n",
      "                              ?0        ?1        ?2\n",
      "Baseline               60.613235  0.280136  2.301062\n",
      "FSN                    60.613235  0.280136  2.301062\n",
      "FairCal (Ours)         60.613235  0.280136  2.301062\n",
      "Oracle (Ours)          60.613235  0.280136  2.301062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/ywvr68ys7ln1r7pltrqvxv4w0000gn/T/ipykernel_2087/754616922.py:42: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n"
     ]
    }
   ],
   "source": [
    "error = [1e-3,1e-2]#,0.5e-1]\n",
    "title_stat = ['AUROC', '0.1\\% FPR', '1\\% FPR']#, '5\\% FPR']\n",
    "n_clusters = 100\n",
    "for calibration in calibration_methods:\n",
    "    row_headers = []\n",
    "    for key in keys:\n",
    "        row_headers.append(title_keys[key])\n",
    "\n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    col_index_lvl_3 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            for i,_ in enumerate(title_stat):\n",
    "                col_index_lvl_1.append(title_datasets[dataset])\n",
    "                col_index_lvl_2.append(title_features[feature])\n",
    "                col_index_lvl_3.append('?'+str(i))\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        if dataset == 'rfw':\n",
    "            att = 'ethnicity'\n",
    "        elif 'bfw' in dataset:\n",
    "            att = 'att'\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for row_i, key in enumerate(keys):\n",
    "                temp = get_overall_stats(calibration, nbins[dataset], dataset,feature,'faircal',att,n_clusters,error)\n",
    "                column_j = len(features_datasets[dataset])*len(title_stat)*i1+len(title_stat)*i2\n",
    "                table[row_i,column_j:column_j+len(title_stat)] = temp.mean(axis=0)\n",
    "\n",
    "    table *= 100\n",
    "    db_temp = pd.DataFrame(\n",
    "            table,\n",
    "            index=row_headers,\n",
    "            columns = col_headers)\n",
    "    print(db_temp)\n",
    "    txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "    for i in range(len(title_stat)):\n",
    "        txt = txt.replace('?'+str(i), title_stat[i])\n",
    "    # Bold entries\n",
    "    txt = txt.split('\\n')\n",
    "    bold_idx = table[:(len(keys)-1),:].argmax(axis=0)\n",
    "    for i, idx in enumerate(bold_idx):\n",
    "        temp = txt[6+idx].split(' & ')\n",
    "        if '\\\\\\\\' in temp[i+1]:\n",
    "            temp[i+1] = '\\\\textbf{'+str(float(temp[i+1][:-2]))+'}\\\\\\\\'\n",
    "        else:\n",
    "            temp[i+1] = '\\\\textbf{'+str(float(temp[i+1]))+'}'\n",
    "\n",
    "        txt[6+idx] = ' & '.join(temp)\n",
    "    txt = '\\n'.join(txt)\n",
    "    txt = txt.replace('{}','',2)\n",
    "    txt = txt.replace('{}','\\hfill $(\\\\uparrow)$')\n",
    "    txt = txt.replace('{tabular}{c}','{tabular}{l'+('|'+('c@{\\hskip 3.5pt}'*len(title_stat)))*4+'}')\n",
    "    txt = txt.replace('multicolumn{8}{c}','multicolumn{8}{c|}',1)\n",
    "    txt = txt.replace('multicolumn{4}{c}','multicolumn{4}{c|}',2)\n",
    "    txt = txt.replace('multicolumn{4}{c|}','multicolumn{4}{c}',1)\n",
    "    txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "    txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "    txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "    txt = txt.replace('\\\\toprule\\n','')\n",
    "    txt = txt.replace('\\\\bottomrule\\n','')\n",
    "    txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "    # Italic entries for Oracle method\n",
    "    txt = txt.split('\\n')\n",
    "    for i in range(table.shape[1]):\n",
    "        temp = txt[4+len(keys)].split(' & ')\n",
    "        if '\\\\\\\\' in temp[i]:\n",
    "            temp[i] = '\\\\textit{'+str(float(temp[i][:-2]))+'}\\\\\\\\'\n",
    "        elif i == 0:\n",
    "            temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "        else:\n",
    "            temp[i] = '\\\\textit{'+str(float(temp[i]))+'}'\n",
    "        txt[4+len(keys)] = ' & '.join(temp)\n",
    "    txt = '\\n'.join(txt)\n",
    "    txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "    label = f\"table:global_performance_{calibration}\"\n",
    "    caption = \"\\\\textbf{Accuracy:} Global accuracy measured by the AUROC and the TPR at different FPR thresholds.\"\n",
    "    txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\centering\\n\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "    with open(f\"../tables_iclr/global_performance/{calibration}.tex\",'w') as file:\n",
    "        file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = [1e-3,1e-2]#,0.5e-1]\n",
    "stats = ['auroc', '1e-3fpr', '1e-2fpr']\n",
    "title_stat = ['Global \\\\textbf{accuracy} measured by the AUROC.',\n",
    "              'Global \\\\textbf{accuracy} measured by the TPR at 0.1\\% FPR threshold.',\n",
    "              'Global \\\\textbf{accuracy} measured by the TPR at 1\\% FPR threshold.']\n",
    "n_clusters = 100\n",
    "for calibration in calibration_methods:\n",
    "    row_headers = []\n",
    "    for key in keys:\n",
    "        row_headers.append(title_keys[key])\n",
    "\n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    col_index_lvl_3 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            for i,_ in enumerate(title_stat):\n",
    "                col_index_lvl_1.append(title_datasets[dataset])\n",
    "                col_index_lvl_2.append(title_features[feature])\n",
    "                col_index_lvl_3.append('?'+str(i))\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "    table_std = np.zeros((len(row_headers),len(col_headers)))\n",
    "\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        if dataset == 'rfw':\n",
    "            att = 'ethnicity'\n",
    "        elif 'bfw' in dataset:\n",
    "            att = 'att'\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for row_i, key in enumerate(keys):\n",
    "                temp = get_overall_stats(calibration, nbins[dataset], dataset,feature,key,att,n_clusters,error)\n",
    "                column_j = len(features_datasets[dataset])*len(title_stat)*i1+len(title_stat)*i2\n",
    "                table[row_i,column_j:column_j+len(title_stat)] = temp.mean(axis=0)\n",
    "                table_std[row_i,column_j:column_j+len(title_stat)] = temp.std(axis=0)\n",
    "\n",
    "    table *= 100\n",
    "    table_std *= 100\n",
    "    \n",
    "    # Setup Column Headers\n",
    "    col_index_lvl_1 = []\n",
    "    col_index_lvl_2 = []\n",
    "    for dataset in datasets:\n",
    "        for feature in features_datasets[dataset]:\n",
    "            col_index_lvl_1.append(title_datasets[dataset])\n",
    "            col_index_lvl_2.append(title_features[feature])\n",
    "\n",
    "    col_headers = [col_index_lvl_1,col_index_lvl_2]\n",
    "    col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "    for i_stat, stat in enumerate(stats):\n",
    "        aux1 = np.apply_along_axis(lambda y: ['%1.2f' % i for i in y], 0, table[:,i_stat::len(stats)])\n",
    "        aux2 = np.apply_along_axis(lambda y: ['@ %1.2f' % i for i in y], 0, table_std[:,i_stat::len(stats)])\n",
    "        table_new = np.core.defchararray.add(aux1, aux2)\n",
    "        db_temp = pd.DataFrame(\n",
    "                table_new,\n",
    "                index=row_headers,\n",
    "                columns=col_headers)            \n",
    "        txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "        txt = txt.replace('@','$\\pm$')\n",
    "\n",
    "\n",
    "        # Bold entries\n",
    "        txt = txt.split('\\n')\n",
    "        bold_idx = table[:,i_stat::len(stats)][:(len(keys)-1),:].argmax(axis=0)\n",
    "        for i, idx in enumerate(bold_idx):\n",
    "            temp = txt[5+idx].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i+1]:\n",
    "                temp[i+1] = '\\\\textbf{'+temp[i+1][:-2]+'}\\\\\\\\'\n",
    "            else:\n",
    "                temp[i+1] = '\\\\textbf{'+temp[i+1]+'}'\n",
    "\n",
    "            txt[5+idx] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        txt = txt.replace('{}','',2)\n",
    "        txt = txt.replace('{}','\\hfill $(\\\\uparrow)$')\n",
    "        txt = txt.replace('{tabular}{c}','{tabular}{l|cc|cc|cc|cc}')\n",
    "        txt = txt.replace('multicolumn{8}{c}','multicolumn{8}{c|}',1)\n",
    "        txt = txt.replace('multicolumn{4}{c}','multicolumn{4}{c|}',2)\n",
    "        txt = txt.replace('multicolumn{4}{c|}','multicolumn{4}{c}',1)\n",
    "        txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "        txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "        txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "        txt = txt.replace('\\\\toprule\\n','')\n",
    "        txt = txt.replace('\\\\bottomrule\\n','')\n",
    "        txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "        # Italic entries for Oracle method\n",
    "        txt = txt.split('\\n')\n",
    "        for i in range(table_new.shape[1]):\n",
    "            temp = txt[3+len(keys)].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i]:\n",
    "                temp[i] = '\\\\textit{'+temp[i][:-2]+'}\\\\\\\\'\n",
    "            elif i == 0:\n",
    "                temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "            else:\n",
    "                temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "            txt[3+len(keys)] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "        label = f\"table:global_performance_{calibration}_{stat}\"\n",
    "        caption = title_stat[i_stat]\n",
    "        txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\centering\\n\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "        with open(f\"../tables_iclr/global_performance/{calibration}_{stat}.tex\",'w') as file:\n",
    "            file.write(txt)\n",
    "            print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-alloy",
   "metadata": {},
   "source": [
    "# Predictive Equality (equal FPR) at different global FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error1_at_error2(calibration_method, nbins, dataset,feature,approach,subgroup,att,n_clusters,errors,at_error):\n",
    "#     if 'bmc' in approach:\n",
    "#         approach = 'faircal'\n",
    "#         temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'_nclusters_'+str(n_clusters)+'.npy',allow_pickle=True).item()\n",
    "#         key = 'calibration'\n",
    "#     elif 'fsn' in approach:\n",
    "#         temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'_nclusters_'+str(n_clusters)+'_fpr_1e-03.npy',allow_pickle=True).item()\n",
    "#         key = 'pre_calibration'\n",
    "#     else:\n",
    "#         if approach == 'fct2':\n",
    "#             temp = np.load('experiments/'+dataset+'_'+feature+'_'+approach+'_'+calibration_method+'_nbins_'+str(nbins)+'.npy',allow_pickle=True).item()\n",
    "#         else:\n",
    "#             temp = np.load('experiments/'+dataset+'/'+feature+'/'+approach+'/'+calibration_method+'/nbins_'+str(nbins)+'.npy',allow_pickle=True).item()\n",
    "        \n",
    "#         if approach == 'oracle':\n",
    "#             key = 'calibration'\n",
    "#         else:\n",
    "#             key = 'pre_calibration'\n",
    "    \n",
    "    if 'bmc' in approach:\n",
    "        approach = 'faircal'\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}.npy\",allow_pickle=True).item()\n",
    "        key = 'calibration'\n",
    "    elif approach == 'faircal':        \n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'fsn':\n",
    "        key = 'pre_calibration'\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}_nclusters_{n_clusters}_fpr_1e-03.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'baseline':\n",
    "        key = 'pre_calibration'\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "    elif approach == 'oracle':\n",
    "        key = 'calibration'\n",
    "        temp = np.load(f\"experiments/{dataset}/{feature}/{approach}/{calibration_method}/nbins_{nbins}.npy\",allow_pickle=True).item()\n",
    "#     elif approach == 'ftc':\n",
    "    \n",
    "    data = np.zeros((5,len(at_error)))\n",
    "    for fold in range(1,6):\n",
    "        if subgroup == 'Global':\n",
    "            fpr = temp['fold'+str(fold)]['fpr'][att][subgroup][key]\n",
    "            tpr = temp['fold'+str(fold)]['tpr'][att][subgroup][key]\n",
    "            \n",
    "            if errors == 'fnr at fpr':\n",
    "                data[fold-1,:] = 1-np.interp(at_error, fpr, tpr)\n",
    "            elif errors == 'fpr at fpr':\n",
    "                data[fold-1,:] = at_error\n",
    "            elif errors == 'fpr at fnr':\n",
    "                data[fold-1,:] = np.interp(1-np.array(at_error), tpr, fpr)\n",
    "            elif errors == 'fnr at fnr':\n",
    "                data[fold-1,:] = at_error\n",
    "        else:\n",
    "            fpr_global = temp['fold'+str(fold)]['fpr'][att]['Global'][key]\n",
    "            tpr_global = temp['fold'+str(fold)]['tpr'][att]['Global'][key]\n",
    "            thr_global = temp['fold'+str(fold)]['thresholds'][att]['Global'][key]\n",
    "            thr_global = np.fmin(thr_global,1)\n",
    "            fpr = temp['fold'+str(fold)]['fpr'][att][subgroup][key]\n",
    "            tpr = temp['fold'+str(fold)]['tpr'][att][subgroup][key]\n",
    "            thr = temp['fold'+str(fold)]['thresholds'][att][subgroup][key]\n",
    "            thr = np.fmin(thr,1)            \n",
    "            if errors == 'fnr at fpr':\n",
    "                thr_at_error = np.interp(at_error,fpr_global,thr_global)\n",
    "                data[fold-1,:] = 1-np.interp(thr_at_error,thr[::-1],tpr[::-1])\n",
    "            elif errors == 'fpr at fpr':\n",
    "                thr_at_error = np.interp(at_error,fpr_global,thr_global)\n",
    "                data[fold-1,:] = np.interp(thr_at_error,thr[::-1],fpr[::-1])\n",
    "            elif errors == 'fpr at fnr':\n",
    "                thr_at_error = np.interp(1-np.array(at_error),tpr_global,thr_global)\n",
    "                data[fold-1,:] = np.interp(thr_at_error,thr[::-1],fpr[::-1])\n",
    "            elif errors == 'fnr at fnr':\n",
    "                thr_at_error = np.interp(1-np.array(at_error),tpr_global,thr_global)\n",
    "                data[fold-1,:] = 1-np.interp(thr_at_error,thr[::-1],tpr[::-1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['aad', 'mad', 'std']\n",
    "at_error = [1e-3,1e-2]\n",
    "n_clusters = 100\n",
    "errors = 'fpr at fpr'\n",
    "title_error = ['0.1\\% FPR', '1\\% FPR']\n",
    "# Setup Row Headers\n",
    "row_index_lvl_1 = []\n",
    "row_index_lvl_2 = []\n",
    "for i, _ in enumerate(title_error):\n",
    "    for key in keys:\n",
    "        row_index_lvl_1.append('?'+str(i))\n",
    "        row_index_lvl_2.append(title_keys[key])\n",
    "\n",
    "row_headers = [row_index_lvl_1,row_index_lvl_2]\n",
    "row_headers = pd.MultiIndex.from_tuples(list(zip(*row_headers)))\n",
    "\n",
    "# Setup Column Headers\n",
    "col_index_lvl_1 = []\n",
    "col_index_lvl_2 = []\n",
    "col_index_lvl_3 = []\n",
    "for dataset in datasets:\n",
    "    for feature in features_datasets[dataset]:\n",
    "        for metric in metrics:\n",
    "            col_index_lvl_1.append(title_datasets[dataset])\n",
    "            col_index_lvl_2.append(title_features[feature])\n",
    "            col_index_lvl_3.append(title_metrics[metric])\n",
    "\n",
    "col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "for calibration in calibration_methods:\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        att = attributes_datasets[dataset]\n",
    "        sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for i3, metric in enumerate(metrics):\n",
    "                for row_i, key in enumerate(keys):\n",
    "                    temp = np.zeros((len(subgroups[att]),5,len(at_error)))\n",
    "                    for ix, subgroup in enumerate(subgroups[att]):\n",
    "                        temp[ix,:,:] = get_error1_at_error2(calibration,nbins[dataset],dataset,feature,key,subgroup,att,n_clusters,errors,at_error)\n",
    "\n",
    "                    data_work = temp.mean(axis=0)\n",
    "                    mean = data_work\n",
    "                    aad = np.mean(np.abs(temp-mean),axis=0)\n",
    "                    mad = np.max(np.abs(temp-mean),axis=0)\n",
    "                    std = np.std(temp,axis=0)\n",
    "                    column_j = len(features_datasets[dataset])*len(metrics)*i1+len(metrics)*i2\n",
    "#                     if i3 == 0:\n",
    "#                         table[row_i::len(keys),column_j+i3] = mean.mean(axis=0)\n",
    "                    if i3 == 0:\n",
    "                        table[row_i::len(keys),column_j+i3] = aad.mean(axis=0)\n",
    "                    if i3 == 1:\n",
    "                        table[row_i::len(keys),column_j+i3] = mad.mean(axis=0)\n",
    "                    if i3 == 2:\n",
    "                        table[row_i::len(keys),column_j+i3] = std.mean(axis=0)\n",
    "                    \n",
    "    table *= 100\n",
    "    db_temp = pd.DataFrame(\n",
    "            table,\n",
    "            index=row_headers,\n",
    "            columns=col_headers)            \n",
    "    txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "    # Bold entries\n",
    "    i_min = 0\n",
    "    i_max = len(keys)-1\n",
    "    start = 6\n",
    "    for i_error in at_error:\n",
    "        txt = txt.split('\\n')\n",
    "        bold_idx = table[i_min:i_max,:].argmin(axis=0)\n",
    "        for i, idx in enumerate(bold_idx):\n",
    "            temp = txt[start+idx].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i+2]:\n",
    "                temp[i+2] = '\\\\textbf{'+str(float(temp[i+2][:-2]))+'}\\\\\\\\'\n",
    "            else:\n",
    "                temp[i+2] = '\\\\textbf{'+str(float(temp[i+2]))+'}'\n",
    "\n",
    "            txt[start+idx] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        i_min += len(keys)\n",
    "        i_max += len(keys)\n",
    "        start += len(keys)\n",
    "    txt = txt.replace('{}','',2)\n",
    "    txt = txt.replace('{}','\\hfill $(\\downarrow)$')\n",
    "    txt = txt.replace('{tabular}{c}','{tabular}{ll|ccc|ccc|ccc|ccc}')\n",
    "    txt = txt.replace('multicolumn{6}{c}','multicolumn{6}{c|}',1)\n",
    "    txt = txt.replace('multicolumn{3}{c}','multicolumn{3}{c|}',2)\n",
    "    txt = txt.replace('multicolumn{3}{c|}','multicolumn{3}{c}',1)\n",
    "    txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "    txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "    txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "    txt = txt.replace('\\\\toprule\\n','')\n",
    "    txt = txt.replace('\\\\bottomrule\\n','')\n",
    "    txt = txt.replace('\\n   & Oracle','[2pt]\\n   & Oracle')\n",
    "    # Italic entries for Oracle method\n",
    "    for i_error in range(len(at_error)):\n",
    "        txt = txt.split('\\n')\n",
    "        temp = txt[10+len(keys)*i_error].split(' & ')\n",
    "        for i in range(1,table.shape[1]+1):\n",
    "            try:\n",
    "                if '\\\\\\\\' in temp[i]:\n",
    "                    temp[i] = '\\\\textit{'+str(float(temp[i][:-2]))+'}\\\\\\\\'\n",
    "                elif i == 1:\n",
    "                    temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "                else:\n",
    "                    temp[i] = '\\\\textit{'+str(float(temp[i]))+'}'\n",
    "            except:\n",
    "                pass\n",
    "            txt[10+len(keys)*i_error] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "    for i in range(len(at_error)):\n",
    "        if i == 0:\n",
    "            txt = txt.replace('?'+str(i), '\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "        else:\n",
    "            txt = txt.replace('?'+str(i), '\\\\midrule\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "    txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "    label = f\"table:predictive_equality_at_fpr_{calibration}\"\n",
    "    caption = \"Predictive equality:Each block of rows represents a choice of global FPR: 0.1\\% and1\\%. For a fixed a global FPR, compare the deviations in subgroup FPRs in terms of three deviation measures: Average Absolute Deviation (AAD), Maximum Absolute Deviation (MAD), and Standard Deviation (STD) (lower is better).\"\n",
    "    txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\\\centering\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "    with open(f\"../tables_iclr/predictive_equality_at_fpr/{calibration}.tex\",'w') as file:\n",
    "        file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['aad', 'mad', 'std']\n",
    "at_error = [1e-3,1e-2]\n",
    "n_clusters = 100\n",
    "errors = 'fpr at fpr'\n",
    "title_error = ['0.1\\% FPR', '1\\% FPR']\n",
    "# Setup Row Headers\n",
    "row_index_lvl_1 = []\n",
    "row_index_lvl_2 = []\n",
    "for i, _ in enumerate(title_error):\n",
    "    for key in keys:\n",
    "        row_index_lvl_1.append('?'+str(i))\n",
    "        row_index_lvl_2.append(title_keys[key])\n",
    "\n",
    "row_headers = [row_index_lvl_1,row_index_lvl_2]\n",
    "row_headers = pd.MultiIndex.from_tuples(list(zip(*row_headers)))\n",
    "\n",
    "# Setup Column Headers\n",
    "col_index_lvl_1 = []\n",
    "col_index_lvl_2 = []\n",
    "for dataset in datasets:\n",
    "    for feature in features_datasets[dataset]:\n",
    "        col_index_lvl_1.append(title_datasets[dataset])\n",
    "        col_index_lvl_2.append(title_features[feature])\n",
    "\n",
    "col_headers = [col_index_lvl_1,col_index_lvl_2]\n",
    "col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "for calibration in calibration_methods:\n",
    "    \n",
    "    table = np.zeros((len(row_headers),len(col_headers)*len(metrics)))\n",
    "    table_std = np.zeros((len(row_headers),len(col_headers)*len(metrics)))\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        att = attributes_datasets[dataset]\n",
    "        sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for i3, metric in enumerate(metrics):\n",
    "                for row_i, key in enumerate(keys):\n",
    "                    temp = np.zeros((len(subgroups[att]),5,len(at_error)))\n",
    "                    for ix, subgroup in enumerate(subgroups[att]):\n",
    "                        temp[ix,:,:] = get_error1_at_error2(calibration,nbins[dataset],dataset,feature,key,subgroup,att,n_clusters,errors,at_error)\n",
    "\n",
    "                    data_work = temp.mean(axis=0)\n",
    "                    mean = data_work\n",
    "                    aad = np.mean(np.abs(temp-mean),axis=0)\n",
    "                    mad = np.max(np.abs(temp-mean),axis=0)\n",
    "                    std = np.std(temp,axis=0)\n",
    "                    column_j = len(features_datasets[dataset])*len(metrics)*i1+len(metrics)*i2\n",
    "                    if i3 == 0:\n",
    "                        table[row_i::len(keys),column_j+i3] = aad.mean(axis=0)\n",
    "                        table_std[row_i::len(keys),column_j+i3] = aad.std(axis=0)\n",
    "                    if i3 == 1:\n",
    "                        table[row_i::len(keys),column_j+i3] = mad.mean(axis=0)\n",
    "                        table_std[row_i::len(keys),column_j+i3] = mad.std(axis=0)\n",
    "                    if i3 == 2:\n",
    "                        table[row_i::len(keys),column_j+i3] = std.mean(axis=0)\n",
    "                        table_std[row_i::len(keys),column_j+i3] = std.std(axis=0)\n",
    "\n",
    "    table *= 100\n",
    "    table_std *= 100\n",
    "    \n",
    "    for i_metric, metric in enumerate(metrics):\n",
    "        aux1 = np.apply_along_axis(lambda y: ['%1.2f' % i for i in y], 0, table[:,i_metric::3])\n",
    "        aux2 = np.apply_along_axis(lambda y: ['@ %1.2f' % i for i in y], 0, table_std[:,i_metric::3])\n",
    "        table_new = np.core.defchararray.add(aux1, aux2)\n",
    "\n",
    "        db_temp = pd.DataFrame(\n",
    "                table_new,\n",
    "                index=row_headers,\n",
    "                columns=col_headers)            \n",
    "        txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "        txt = txt.replace('@','$\\pm$')\n",
    "        # Bold entries\n",
    "        i_min = 0\n",
    "        i_max = len(keys)-1\n",
    "        start = 5\n",
    "        for i_error in at_error:\n",
    "            txt = txt.split('\\n')\n",
    "            bold_idx = table[:,i_metric::3][i_min:i_max,:].argmin(axis=0)\n",
    "            for i, idx in enumerate(bold_idx):\n",
    "                temp = txt[start+idx].split(' & ')\n",
    "                if '\\\\\\\\' in temp[i+2]:\n",
    "                    temp[i+2] = '\\\\textbf{'+temp[i+2][:-2]+'}\\\\\\\\'\n",
    "                else:\n",
    "                    temp[i+2] = '\\\\textbf{'+temp[i+2]+'}'\n",
    "\n",
    "                txt[start+idx] = ' & '.join(temp)\n",
    "            txt = '\\n'.join(txt)\n",
    "            i_min += len(keys)\n",
    "            i_max += len(keys)\n",
    "            start += len(keys)\n",
    "    #     txt = txt.replace('{}','',2)\n",
    "    #     txt = txt.replace('{}','\\hfill $(\\downarrow)$')\n",
    "        txt = txt.replace('{tabular}{c}','{tabular}{ll|cc|cc}')\n",
    "        txt = txt.replace('multicolumn{2}{c}','multicolumn{2}{c|}',1)\n",
    "#         txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "#         txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "#         txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "        txt = txt.replace('FairCal (Ours)','\\\\textbf{FairCal (Ours)}')\n",
    "        txt = txt.replace('\\\\toprule\\n','')\n",
    "        txt = txt.replace('\\\\bottomrule\\n','')\n",
    "        txt = txt.replace('\\n   & Oracle','[2pt]\\n   & Oracle')\n",
    "        # Italic entries for Oracle method\n",
    "        for i_error in range(len(at_error)):\n",
    "            txt = txt.split('\\n')\n",
    "            temp = txt[9+len(keys)*i_error].split(' & ')\n",
    "            for i in range(1,table_new.shape[1]+1):\n",
    "                if '\\\\\\\\' in temp[i]:\n",
    "                    temp[i] = '\\\\textit{'+temp[i][:-2]+'}\\\\\\\\'\n",
    "                elif i == 1:\n",
    "                    temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "                else:\n",
    "                    temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "                txt[9+len(keys)*i_error] = ' & '.join(temp)\n",
    "            txt = '\\n'.join(txt)\n",
    "        for i in range(len(at_error)):\n",
    "            if i == 0:\n",
    "                txt = txt.replace('?'+str(i), '\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "            else:\n",
    "                txt = txt.replace('?'+str(i), '\\\\midrule\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "        txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "        label = f\"table:predictive_equality_at_fpr_{calibration}_{metric}\"\n",
    "        caption = \"\\\\textbf{Predictive equality:} Each block of rows represents a choice of global FPR: 0.1\\% and 1\\%. For a fixed a global FPR, compare the deviations in subgroup FPRs in terms of \"+caption_metrics[metric]+\". We report the average and standard deviation error across the 5 folds.\"\n",
    "        txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\\\centering\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt  \n",
    "        with open(f\"../tables_iclr/predictive_equality_at_fpr/{calibration}_{metric}.tex\",'w') as file:\n",
    "            file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-monroe",
   "metadata": {},
   "source": [
    "# Equal Opportunity (equal FNR) at different global FNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8babd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['aad', 'mad', 'std']\n",
    "at_error = [1e-3,1e-2]\n",
    "n_clusters = 100\n",
    "errors = 'fnr at fnr'\n",
    "title_error = ['0.1\\% FNR', '1\\% FNR']\n",
    "# Setup Row Headers\n",
    "row_index_lvl_1 = []\n",
    "row_index_lvl_2 = []\n",
    "for i, _ in enumerate(title_error):\n",
    "    for key in keys:\n",
    "        row_index_lvl_1.append('?'+str(i))\n",
    "        row_index_lvl_2.append(title_keys[key])\n",
    "\n",
    "row_headers = [row_index_lvl_1,row_index_lvl_2]\n",
    "row_headers = pd.MultiIndex.from_tuples(list(zip(*row_headers)))\n",
    "\n",
    "# Setup Column Headers\n",
    "col_index_lvl_1 = []\n",
    "col_index_lvl_2 = []\n",
    "col_index_lvl_3 = []\n",
    "for dataset in datasets:\n",
    "    for feature in features_datasets[dataset]:\n",
    "        for metric in metrics:\n",
    "            col_index_lvl_1.append(title_datasets[dataset])\n",
    "            col_index_lvl_2.append(title_features[feature])\n",
    "            col_index_lvl_3.append(title_metrics[metric])\n",
    "\n",
    "col_headers = [col_index_lvl_1,col_index_lvl_2,col_index_lvl_3]\n",
    "col_headers = pd.MultiIndex.from_tuples(list(zip(*col_headers)))\n",
    "\n",
    "for calibration in calibration_methods:\n",
    "\n",
    "    table = np.zeros((len(row_headers),len(col_headers)))\n",
    "    for i1, dataset in enumerate(datasets):\n",
    "        att = attributes_datasets[dataset]\n",
    "        sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "        for i2, feature in enumerate(features_datasets[dataset]):\n",
    "            for i3, metric in enumerate(metrics):\n",
    "                for row_i, key in enumerate(keys):\n",
    "                    temp = np.zeros((len(subgroups[att]),5,len(at_error)))\n",
    "                    for ix, subgroup in enumerate(subgroups[att]):\n",
    "                        temp[ix,:,:] = get_error1_at_error2(calibration,nbins[dataset],dataset,feature,key,subgroup,att,n_clusters,errors,at_error)\n",
    "\n",
    "                    data_work = temp.mean(axis=0)\n",
    "                    mean = data_work\n",
    "                    aad = np.mean(np.abs(temp-mean),axis=0)\n",
    "                    mad = np.max(np.abs(temp-mean),axis=0)\n",
    "                    std = np.std(temp,axis=0)\n",
    "                    column_j = len(features_datasets[dataset])*len(metrics)*i1+len(metrics)*i2\n",
    "                    if i3 == 0:\n",
    "                        table[row_i::len(keys),column_j+i3] = aad.mean(axis=0)\n",
    "                    if i3 == 1:\n",
    "                        table[row_i::len(keys),column_j+i3] = mad.mean(axis=0)\n",
    "                    if i3 == 2:\n",
    "                        table[row_i::len(keys),column_j+i3] = std.mean(axis=0)\n",
    "                    \n",
    "    table *= 100\n",
    "    db_temp = pd.DataFrame(\n",
    "            table,\n",
    "            index=row_headers,\n",
    "            columns=col_headers)            \n",
    "    txt = db_temp.to_latex(float_format=\"%.2f\",multicolumn_format='c', column_format='c')\n",
    "    # Bold entries\n",
    "    i_min = 0\n",
    "    i_max = len(keys)-1\n",
    "    start = 6\n",
    "    for i_error in at_error:\n",
    "        txt = txt.split('\\n')\n",
    "        bold_idx = table[i_min:i_max,:].argmin(axis=0)\n",
    "        for i, idx in enumerate(bold_idx):\n",
    "            temp = txt[start+idx].split(' & ')\n",
    "            if '\\\\\\\\' in temp[i+2]:\n",
    "                temp[i+2] = '\\\\textbf{'+str(float(temp[i+2][:-2]))+'}\\\\\\\\'\n",
    "            else:\n",
    "                temp[i+2] = '\\\\textbf{'+str(float(temp[i+2]))+'}'\n",
    "\n",
    "            txt[start+idx] = ' & '.join(temp)\n",
    "        txt = '\\n'.join(txt)\n",
    "        i_min += len(keys)\n",
    "        i_max += len(keys)\n",
    "        start += len(keys)\n",
    "    txt = txt.replace('{}','',2)\n",
    "    txt = txt.replace('{}','\\hfill $(\\downarrow)$')\n",
    "    txt = txt.replace('{tabular}{c}','{tabular}{ll|ccc|ccc|ccc|ccc}')\n",
    "    txt = txt.replace('multicolumn{6}{c}','multicolumn{6}{c|}',1)\n",
    "    txt = txt.replace('multicolumn{3}{c}','multicolumn{3}{c|}',2)\n",
    "    txt = txt.replace('multicolumn{3}{c|}','multicolumn{3}{c}',1)\n",
    "    txt = txt.replace('AGENDA','AGENDA \\citep{Dhar2020AGENDA}')\n",
    "    txt = txt.replace('FTC','FTC \\citep{Terhorst2020Comparison}')\n",
    "    txt = txt.replace('FSN','FSN \\citep{terhorst2020scorenormalization}')\n",
    "    txt = txt.replace('\\\\toprule\\n','')\n",
    "    txt = txt.replace('\\\\bottomrule\\n','')\n",
    "    txt = txt.replace('\\nOracle','[2pt]\\nOracle')\n",
    "    # Italic entries for Oracle method\n",
    "    for i_error in range(len(at_error)):\n",
    "        txt = txt.split('\\n')\n",
    "        temp = txt[10+len(keys)*i_error].split(' & ')\n",
    "        for i in range(1,table.shape[1]+1):\n",
    "            try:\n",
    "                if '\\\\\\\\' in temp[i]:\n",
    "                    temp[i] = '\\\\textit{'+str(float(temp[i][:-2]))+'}\\\\\\\\'\n",
    "                elif i == 1:\n",
    "                    temp[i] = '\\\\textit{'+temp[i]+'}'\n",
    "                else:\n",
    "                    temp[i] = '\\\\textit{'+str(float(temp[i]))+'}'\n",
    "                txt[10+len(keys)*i_error] = ' & '.join(temp)\n",
    "            except:\n",
    "                pass\n",
    "        txt = '\\n'.join(txt)\n",
    "    for i in range(len(at_error)):\n",
    "        if i == 0:\n",
    "            txt = txt.replace('?'+str(i), '\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "        else:\n",
    "            txt = txt.replace('?'+str(i), '\\\\midrule\\\\multirow{'+str(len(keys))+'}{*}{\\\\rotatebox{90}{'+title_error[i]+'}}',1)\n",
    "    txt = txt.replace('end{tabular}','end{tabular}}\\n\\\\end{table}')\n",
    "    label = f\"table:equal_opportunity_at_fnr_{calibration}\"\n",
    "    caption = f'Equal opportunity (equal FNR) at different global FNRs across across the sensitive subgroups.'\n",
    "    txt = \"\\\\begin{table}\\n\\\\caption{\"+caption+\"}\\n\\\\label{\"+label+\"}\\n\\\\centering\\\\resizebox{\\\\textwidth}{!}{\\n\"+txt\n",
    "    with open(f\"../tables_iclr/equal_opportunity_at_fnr/{calibration}.tex\",'w') as file:\n",
    "        file.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-touch",
   "metadata": {},
   "source": [
    "# Creating Figures Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.array([1,5,10,15,20,25,50,75,100,150,250,500])\n",
    "folds = [1,2,3,4,5]\n",
    "datasets = ['rfw', 'bfw']\n",
    "mean = {}\n",
    "aad = {}\n",
    "mad = {}\n",
    "std = {}\n",
    "keys = ['baseline', 'fsn', 'bmc', 'oracle']\n",
    "for measure in measures:\n",
    "    mean[measure] = {}\n",
    "    aad[measure] = {}\n",
    "    mad[measure] = {}\n",
    "    std[measure] = {}\n",
    "    for calibration in ['beta', 'binning', 'isotonic_regression']:\n",
    "        mean[measure][calibration] = {}\n",
    "        aad[measure][calibration] = {}\n",
    "        mad[measure][calibration] = {}\n",
    "        std[measure][calibration] = {}\n",
    "        for dataset in datasets:\n",
    "            mean[measure][calibration][dataset] = {}\n",
    "            aad[measure][calibration][dataset] = {}\n",
    "            mad[measure][calibration][dataset] = {}\n",
    "            std[measure][calibration][dataset] = {}\n",
    "            sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "            for feature in features:\n",
    "                mean[measure][calibration][dataset][feature] = {}\n",
    "                aad[measure][calibration][dataset][feature] = {}\n",
    "                mad[measure][calibration][dataset][feature] = {}\n",
    "                std[measure][calibration][dataset][feature] = {}\n",
    "                for att in sensitive_attributes:\n",
    "                    mean[measure][calibration][dataset][feature][att] = {}\n",
    "                    aad[measure][calibration][dataset][feature][att] = {}\n",
    "                    mad[measure][calibration][dataset][feature][att] = {}\n",
    "                    std[measure][calibration][dataset][feature][att] = {}\n",
    "                    for key in keys:\n",
    "                        mean[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        aad[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        mad[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        std[measure][calibration][dataset][feature][att][key] = np.zeros((len(folds),len(ks)))\n",
    "                        data_work = np.zeros((len(folds),len(subgroups[att])))\n",
    "                        for i_k, k in enumerate(ks):\n",
    "                            data_work = load_measures(dataset,feature,key,subgroups,att,measure,calibration,nbins[dataset],k)\n",
    "                            if key != 'brier':\n",
    "                                data_work *= 100\n",
    "                            mean_aux = data_work.mean(axis=1).reshape(-1,1)\n",
    "                            mean[measure][calibration][dataset][feature][att][key][:,i_k] = data_work.mean(axis=1)\n",
    "                            aad[measure][calibration][dataset][feature][att][key][:,i_k] = np.abs(data_work-mean_aux).mean(axis=1)\n",
    "                            mad[measure][calibration][dataset][feature][att][key][:,i_k] = np.abs(data_work-mean_aux).max(axis=1)\n",
    "                            std[measure][calibration][dataset][feature][att][key][:,i_k] = np.std(data_work,axis=1)\n",
    "data = {'mean':mean, 'aad':aad, 'mad': mad, 'std': std}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['mean', 'aad', 'mad', 'std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'bmc'\n",
    "for measure in ['ks']:#measures:\n",
    "    for metric in metrics:\n",
    "        total_graphs = 0\n",
    "        for dataset in datasets:\n",
    "            total_graphs += len(features_datasets[dataset])\n",
    "        fig, ax = plt.subplots(1,total_graphs, figsize=(total_graphs*6,5), sharey=True)\n",
    "        ix = 0\n",
    "        for dataset in datasets:\n",
    "            sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "            att = attributes_datasets[dataset]\n",
    "            for feature in features_datasets[dataset]:\n",
    "                legend = []\n",
    "                for calibration in ['beta','binning','isotonic_regression']:\n",
    "                    y = data[metric][measure][calibration][dataset][feature][att][key].mean(axis=0)\n",
    "                    y_std = data[metric][measure][calibration][dataset][feature][att][key].std(axis=0)\n",
    "                    ax[ix].plot(np.arange(len(ks)),y,marker='o')\n",
    "                    ax[ix].fill_between(np.arange(len(ks)),y-y_std,y+y_std,alpha = 0.5)\n",
    "                    legend.append(title_calibration_methods[calibration])\n",
    "                ax[ix].legend(legend, fontsize=legend_fontsize)\n",
    "                ax[ix].set_xlabel('K', fontsize=label_fontsize)\n",
    "                ax[ix].xaxis.set_ticks(np.arange(len(ks)))\n",
    "                ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "                ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "                ax[ix].tick_params(axis='both', which='major', labelsize=ticks_fontsize)\n",
    "                ax[ix].set_title(f\"{title_datasets[dataset]} - {title_features[feature]}\", fontsize=title_fontsize)\n",
    "                ix += 1\n",
    "        save_as = f\"{measure}_{metric}.pdf\"\n",
    "        fig.tight_layout()\n",
    "        plt.savefig('../figs_iclr/robustness/'+save_as, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'bmc'\n",
    "calibration = 'beta'\n",
    "for measure in ['ks']:#measures:\n",
    "    for metric in metrics:\n",
    "        total_graphs = 0\n",
    "        for dataset in datasets:\n",
    "            total_graphs += len(features_datasets[dataset])\n",
    "        fig, ax = plt.subplots(1,total_graphs, figsize=(total_graphs*6,5), sharey=True)\n",
    "        ix = 0\n",
    "        for dataset in datasets:\n",
    "            sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "            att = attributes_datasets[dataset]\n",
    "            for feature in features_datasets[dataset]:\n",
    "                legend = []\n",
    "                for key in ['baseline', 'fsn', 'bmc', 'oracle']:\n",
    "                    y = data[metric][measure][calibration][dataset][feature][att][key].mean(axis=0)\n",
    "                    y_std = data[metric][measure][calibration][dataset][feature][att][key].std(axis=0)\n",
    "                    ax[ix].plot(np.arange(len(ks)),y,marker='o')\n",
    "                    ax[ix].fill_between(np.arange(len(ks)),y-y_std,y+y_std,alpha = 0.5)\n",
    "                    legend.append(title_keys[key])\n",
    "                ax[ix].legend(legend, fontsize=legend_fontsize)\n",
    "                ax[ix].set_xlabel('K', fontsize=label_fontsize)\n",
    "                ax[ix].xaxis.set_ticks(np.arange(len(ks)))\n",
    "                ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "                ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "                ax[ix].tick_params(axis='both', which='major', labelsize=ticks_fontsize)\n",
    "                ax[ix].set_title(f\"{title_datasets[dataset]} - {title_features[feature]}\", fontsize=title_fontsize)\n",
    "                ix += 1\n",
    "        save_as = f\"{measure}_{metric}_vs_other_methods.pdf\"\n",
    "        fig.tight_layout()\n",
    "        plt.savefig('../figs_iclr/robustness/'+save_as, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.array([1,5,10,15,20,25,50,75,100,150,250,500])\n",
    "fpr_def = [1e-3, 1e-2]\n",
    "key = 'bmc'\n",
    "calibration = 'beta'\n",
    "stats = ['auroc', '1e-3fpr', '1e-2fpr']\n",
    "for i_stat, stat in enumerate(stats):\n",
    "    total_graphs = 0\n",
    "    for dataset in datasets:\n",
    "        total_graphs += len(features_datasets[dataset])\n",
    "    fig, ax = plt.subplots(1,total_graphs, figsize=(total_graphs*7,5))#, sharey=True)\n",
    "    ix = 0\n",
    "    for dataset in datasets:\n",
    "        sensitive_attributes, subgroups = get_sensitive_attributes_subgroups(dataset)\n",
    "        att = attributes_datasets[dataset]\n",
    "        for feature in features_datasets[dataset]:\n",
    "            legend = []\n",
    "            for key in ['baseline', 'fsn', 'bmc', 'oracle']:\n",
    "                temp = np.zeros((5,len(ks)))\n",
    "                for i_k, k in enumerate(ks):\n",
    "                    temp_aux = get_overall_stats(calibration, nbins[dataset], dataset,feature, key, att, k, fpr_def)\n",
    "                    temp[:,i_k] = temp_aux[:,i_stat]\n",
    "                y = temp.mean(axis=0)\n",
    "                y_std = temp.std(axis=0)\n",
    "                ax[ix].plot(np.arange(len(ks)),y,marker='o')\n",
    "                ax[ix].fill_between(np.arange(len(ks)),y-y_std,y+y_std,alpha = 0.5)\n",
    "                legend.append(title_keys[key])\n",
    "            ax[ix].legend(legend, fontsize=legend_fontsize)\n",
    "            ax[ix].set_xlabel('K', fontsize=label_fontsize)\n",
    "            ax[ix].xaxis.set_ticks(np.arange(len(ks)))\n",
    "            ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "            ax[ix].xaxis.set_ticklabels(ks, fontsize=ticks_fontsize)\n",
    "            ax[ix].tick_params(axis='both', which='major', labelsize=ticks_fontsize)\n",
    "            ax[ix].set_title(f\"{title_datasets[dataset]} - {title_features[feature]}\", fontsize=title_fontsize)\n",
    "            ix += 1\n",
    "    save_as = f\"{measure}_{metric}_{stat}_vs_other_methods.pdf\"\n",
    "    fig.tight_layout()\n",
    "    plt.savefig('../figs_iclr/robustness/'+save_as, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-copper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-receptor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-spyware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-squad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-mobility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-router",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
